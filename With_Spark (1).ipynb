{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyspark\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/87/21/f05c186f4ddb01d15d0ddc36ef4b7e3cedbeb6412274a41f26b55a650ee5/pyspark-2.4.4.tar.gz (215.7MB)\n",
      "\u001b[K    100% |████████████████████████████████| 215.7MB 192kB/s \n",
      "\u001b[?25hCollecting py4j==0.10.7 (from pyspark)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e3/53/c737818eb9a7dc32a7cd4f1396e787bd94200c3997c72c1dbe028587bd76/py4j-0.10.7-py2.py3-none-any.whl (197kB)\n",
      "\u001b[K    100% |████████████████████████████████| 204kB 32.0MB/s \n",
      "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
      "  Running setup.py bdist_wheel for pyspark ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/ab/09/4d/0d184230058e654eb1b04467dbc1292f00eaa186544604b471\n",
      "Successfully built pyspark\n",
      "\u001b[31mscikit-umfpack 0.3.2 has requirement numpy>=1.15.3, but you'll have numpy 1.15.1 which is incompatible.\u001b[0m\n",
      "\u001b[31mmenpo 0.8.1 has requirement matplotlib<2.0,>=1.4, but you'll have matplotlib 2.2.3 which is incompatible.\u001b[0m\n",
      "\u001b[31mmenpo 0.8.1 has requirement pillow<5.0,>=3.0, but you'll have pillow 5.2.0 which is incompatible.\u001b[0m\n",
      "\u001b[31mmenpo 0.8.1 has requirement scipy<1.0,>=0.16, but you'll have scipy 1.1.0 which is incompatible.\u001b[0m\n",
      "Installing collected packages: py4j, pyspark\n",
      "Successfully installed py4j-0.10.7 pyspark-2.4.4\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 19.2.3 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Collecting bs4\n",
      "  Downloading https://files.pythonhosted.org/packages/10/ed/7e8b97591f6f456174139ec089c769f89a94a1a4025fe967691de971f314/bs4-0.0.1.tar.gz\n",
      "Collecting beautifulsoup4 (from bs4)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1a/b7/34eec2fe5a49718944e215fde81288eec1fa04638aa3fb57c1c6cd0f98c3/beautifulsoup4-4.8.0-py3-none-any.whl (97kB)\n",
      "\u001b[K    100% |████████████████████████████████| 102kB 5.8MB/s \n",
      "\u001b[?25hCollecting soupsieve>=1.2 (from beautifulsoup4->bs4)\n",
      "  Downloading https://files.pythonhosted.org/packages/0b/44/0474f2207fdd601bb25787671c81076333d2c80e6f97e92790f8887cf682/soupsieve-1.9.3-py2.py3-none-any.whl\n",
      "Building wheels for collected packages: bs4\n",
      "  Running setup.py bdist_wheel for bs4 ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/a0/b0/b2/4f80b9456b87abedbc0bf2d52235414c3467d8889be38dd472\n",
      "Successfully built bs4\n",
      "\u001b[31mscikit-umfpack 0.3.2 has requirement numpy>=1.15.3, but you'll have numpy 1.15.1 which is incompatible.\u001b[0m\n",
      "\u001b[31mmenpo 0.8.1 has requirement matplotlib<2.0,>=1.4, but you'll have matplotlib 2.2.3 which is incompatible.\u001b[0m\n",
      "\u001b[31mmenpo 0.8.1 has requirement pillow<5.0,>=3.0, but you'll have pillow 5.2.0 which is incompatible.\u001b[0m\n",
      "\u001b[31mmenpo 0.8.1 has requirement scipy<1.0,>=0.16, but you'll have scipy 1.1.0 which is incompatible.\u001b[0m\n",
      "Installing collected packages: soupsieve, beautifulsoup4, bs4\n",
      "Successfully installed beautifulsoup4-4.8.0 bs4-0.0.1 soupsieve-1.9.3\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 19.2.3 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip install pyspark\n",
    "!pip install bs4\n",
    "import nltk \n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from keras.preprocessing.text import Tokenizer \n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from nltk.corpus import stopwords\n",
    "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Concatenate, TimeDistributed\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import warnings\n",
    "pd.set_option(\"display.max_colwidth\", 200)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql import SparkSession, SQLContext\n",
    "from pyspark import SparkConf, SparkContext\n",
    "import os\n",
    "from pyspark.ml import Pipeline\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "from tensorflow.python.keras.layers import Layer\n",
    "from tensorflow.python.keras import backend as K\n",
    "\n",
    "\n",
    "class AttentionLayer(Layer):\n",
    "    \"\"\"\n",
    "    This class implements Bahdanau attention (https://arxiv.org/pdf/1409.0473.pdf).\n",
    "    There are three sets of weights introduced W_a, U_a, and V_a\n",
    "     \"\"\"\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super(AttentionLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert isinstance(input_shape, list)\n",
    "        # Create a trainable weight variable for this layer.\n",
    "\n",
    "        self.W_a = self.add_weight(name='W_a',\n",
    "                                   shape=tf.TensorShape((input_shape[0][2], input_shape[0][2])),\n",
    "                                   initializer='uniform',\n",
    "                                   trainable=True)\n",
    "        self.U_a = self.add_weight(name='U_a',\n",
    "                                   shape=tf.TensorShape((input_shape[1][2], input_shape[0][2])),\n",
    "                                   initializer='uniform',\n",
    "                                   trainable=True)\n",
    "        self.V_a = self.add_weight(name='V_a',\n",
    "                                   shape=tf.TensorShape((input_shape[0][2], 1)),\n",
    "                                   initializer='uniform',\n",
    "                                   trainable=True)\n",
    "\n",
    "        super(AttentionLayer, self).build(input_shape)  # Be sure to call this at the end\n",
    "\n",
    "    def call(self, inputs, verbose=False):\n",
    "        \"\"\"\n",
    "        inputs: [encoder_output_sequence, decoder_output_sequence]\n",
    "        \"\"\"\n",
    "        assert type(inputs) == list\n",
    "        encoder_out_seq, decoder_out_seq = inputs\n",
    "        if verbose:\n",
    "            print('encoder_out_seq>', encoder_out_seq.shape)\n",
    "            print('decoder_out_seq>', decoder_out_seq.shape)\n",
    "\n",
    "        def energy_step(inputs, states):\n",
    "            \"\"\" Step function for computing energy for a single decoder state \"\"\"\n",
    "\n",
    "            assert_msg = \"States must be a list. However states {} is of type {}\".format(states, type(states))\n",
    "            assert isinstance(states, list) or isinstance(states, tuple), assert_msg\n",
    "\n",
    "            \"\"\" Some parameters required for shaping tensors\"\"\"\n",
    "            en_seq_len, en_hidden = encoder_out_seq.shape[1], encoder_out_seq.shape[2]\n",
    "            de_hidden = inputs.shape[-1]\n",
    "\n",
    "            \"\"\" Computing S.Wa where S=[s0, s1, ..., si]\"\"\"\n",
    "            # <= batch_size*en_seq_len, latent_dim\n",
    "            reshaped_enc_outputs = K.reshape(encoder_out_seq, (-1, en_hidden))\n",
    "            # <= batch_size*en_seq_len, latent_dim\n",
    "            W_a_dot_s = K.reshape(K.dot(reshaped_enc_outputs, self.W_a), (-1, en_seq_len, en_hidden))\n",
    "            if verbose:\n",
    "                print('wa.s>',W_a_dot_s.shape)\n",
    "\n",
    "            \"\"\" Computing hj.Ua \"\"\"\n",
    "            U_a_dot_h = K.expand_dims(K.dot(inputs, self.U_a), 1)  # <= batch_size, 1, latent_dim\n",
    "            if verbose:\n",
    "                print('Ua.h>',U_a_dot_h.shape)\n",
    "\n",
    "            \"\"\" tanh(S.Wa + hj.Ua) \"\"\"\n",
    "            # <= batch_size*en_seq_len, latent_dim\n",
    "            reshaped_Ws_plus_Uh = K.tanh(K.reshape(W_a_dot_s + U_a_dot_h, (-1, en_hidden)))\n",
    "            if verbose:\n",
    "                print('Ws+Uh>', reshaped_Ws_plus_Uh.shape)\n",
    "\n",
    "            \"\"\" softmax(va.tanh(S.Wa + hj.Ua)) \"\"\"\n",
    "            # <= batch_size, en_seq_len\n",
    "            e_i = K.reshape(K.dot(reshaped_Ws_plus_Uh, self.V_a), (-1, en_seq_len))\n",
    "            # <= batch_size, en_seq_len\n",
    "            e_i = K.softmax(e_i)\n",
    "\n",
    "            if verbose:\n",
    "                print('ei>', e_i.shape)\n",
    "\n",
    "            return e_i, [e_i]\n",
    "\n",
    "        def context_step(inputs, states):\n",
    "            \"\"\" Step function for computing ci using ei \"\"\"\n",
    "            # <= batch_size, hidden_size\n",
    "            c_i = K.sum(encoder_out_seq * K.expand_dims(inputs, -1), axis=1)\n",
    "            if verbose:\n",
    "                print('ci>', c_i.shape)\n",
    "            return c_i, [c_i]\n",
    "\n",
    "        def create_inital_state(inputs, hidden_size):\n",
    "            # We are not using initial states, but need to pass something to K.rnn funciton\n",
    "            fake_state = K.zeros_like(inputs)  # <= (batch_size, enc_seq_len, latent_dim\n",
    "            fake_state = K.sum(fake_state, axis=[1, 2])  # <= (batch_size)\n",
    "            fake_state = K.expand_dims(fake_state)  # <= (batch_size, 1)\n",
    "            fake_state = K.tile(fake_state, [1, hidden_size])  # <= (batch_size, latent_dim\n",
    "            return fake_state\n",
    "\n",
    "        fake_state_c = create_inital_state(encoder_out_seq, encoder_out_seq.shape[-1])\n",
    "        fake_state_e = create_inital_state(encoder_out_seq, encoder_out_seq.shape[1])  # <= (batch_size, enc_seq_len, latent_dim\n",
    "\n",
    "        \"\"\" Computing energy outputs \"\"\"\n",
    "        # e_outputs => (batch_size, de_seq_len, en_seq_len)\n",
    "        last_out, e_outputs, _ = K.rnn(\n",
    "            energy_step, decoder_out_seq, [fake_state_e],\n",
    "        )\n",
    "\n",
    "        \"\"\" Computing context vectors \"\"\"\n",
    "        last_out, c_outputs, _ = K.rnn(\n",
    "            context_step, e_outputs, [fake_state_c],\n",
    "        )\n",
    "\n",
    "        return c_outputs, e_outputs\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        \"\"\" Outputs produced by the layer \"\"\"\n",
    "        return [\n",
    "            tf.TensorShape((input_shape[1][0], input_shape[1][1], input_shape[1][2])),\n",
    "            tf.TensorShape((input_shape[1][0], input_shape[1][1], input_shape[0][1]))\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "SystemError",
     "evalue": "GPU device not found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mSystemError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-ce08ddae0a7d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdevice_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgpu_device_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdevice_name\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'/device:GPU:0'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m   \u001b[0;32mraise\u001b[0m \u001b[0mSystemError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'GPU device not found'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Found GPU at: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mSystemError\u001b[0m: GPU device not found"
     ]
    }
   ],
   "source": [
    "device_name = tf.test.gpu_device_name()\n",
    "if device_name != '/device:GPU:0':\n",
    "  raise SystemError('GPU device not found')\n",
    "print('Found GPU at: {}'.format(device_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"EToSA\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.conf.set(\"spark.sql.execution.arrow.enabled\", \"true\")\n",
    "\n",
    "sc = SparkContext.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://floydhub:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.4</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>EToSA</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=EToSA>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import keyword_only\n",
    "from pyspark.ml import Transformer\n",
    "from pyspark.ml.param.shared import HasInputCol, HasOutputCol, HasRegParam\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert to lower\n",
    "class ConvertToLower(Transformer, HasInputCol, HasOutputCol):\n",
    "\n",
    "    @keyword_only\n",
    "    def __init__(self, inputCol=None, outputCol=None):\n",
    "        super(ConvertToLower, self).__init__()\n",
    "        kwargs = self._input_kwargs\n",
    "        self.setParams(**kwargs)\n",
    "\n",
    "    @keyword_only\n",
    "    def setParams(self, inputCol=None, outputCol=None):\n",
    "        kwargs = self._input_kwargs\n",
    "        return self._set(**kwargs)\n",
    "\n",
    "    def _transform(self, dataset):\n",
    "\n",
    "        def f(s):\n",
    "            cleaned_post = s.lower()\n",
    "            return cleaned_post\n",
    "\n",
    "        t = StringType()\n",
    "        out_col = self.getOutputCol()\n",
    "        in_col = dataset[self.getInputCol()]\n",
    "        return dataset.withColumn(out_col, udf(f, t)(in_col))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RemoveHTMLTags(Transformer, HasInputCol, HasOutputCol):\n",
    "\n",
    "    @keyword_only\n",
    "    def __init__(self, inputCol=None, outputCol=None):\n",
    "        super(RemoveHTMLTags, self).__init__()\n",
    "        kwargs = self._input_kwargs\n",
    "        self.setParams(**kwargs)\n",
    "\n",
    "    @keyword_only\n",
    "    def setParams(self, inputCol=None, outputCol=None):\n",
    "        kwargs = self._input_kwargs\n",
    "        return self._set(**kwargs)\n",
    "\n",
    "    def _transform(self, dataset):\n",
    "\n",
    "        def f(s):\n",
    "            newString = s\n",
    "            newString = BeautifulSoup(newString, \"html\").text\n",
    "            cleaned_post = newString\n",
    "            return cleaned_post\n",
    "\n",
    "        t = StringType()\n",
    "        out_col = self.getOutputCol()\n",
    "        in_col = dataset[self.getInputCol()]\n",
    "        return dataset.withColumn(out_col, udf(f, t)(in_col))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Contraction Mapping\n",
    "contraction_mapping = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\",\n",
    "                           \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\",\n",
    "                           \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",\n",
    "                           \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\",\n",
    "                           \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\",\n",
    "                           \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\",\n",
    "                           \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\",\n",
    "                           \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\",\n",
    "                           \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\",\n",
    "                           \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\",\n",
    "                           \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\",\n",
    "                           \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\",\n",
    "                           \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\",\n",
    "                           \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\",\n",
    "                           \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\",\n",
    "                           \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",\n",
    "                           \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\",\n",
    "                           \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\",\n",
    "                           \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\",\n",
    "                           \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\",\n",
    "                           \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\n",
    "                           \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\",\n",
    "                           \"you're\": \"you are\", \"you've\": \"you have\"}\n",
    "\n",
    "class RemoveUnwantedCharacters(Transformer, HasInputCol, HasOutputCol):\n",
    "\n",
    "    @keyword_only\n",
    "    def __init__(self, inputCol=None, outputCol=None):\n",
    "        super(RemoveUnwantedCharacters, self).__init__()\n",
    "        kwargs = self._input_kwargs\n",
    "        self.setParams(**kwargs)\n",
    "\n",
    "    @keyword_only\n",
    "    def setParams(self, inputCol=None, outputCol=None):\n",
    "        kwargs = self._input_kwargs\n",
    "        return self._set(**kwargs)\n",
    "\n",
    "    def _transform(self, dataset):\n",
    "\n",
    "        def f(s):\n",
    "            newString = s\n",
    "            newString = re.sub(r'\\([^)]*\\)', '', newString) # text between parenthesis\n",
    "            newString = re.sub('\"','', newString) #punctuation\n",
    "            newString = ' '.join([contraction_mapping[t] if t in contraction_mapping else t for t in newString.split(\" \")])    \n",
    "            newString = re.sub(r\"'s\\b\",\"\",newString) #apostrophe\n",
    "            newString = re.sub(\"[^a-zA-Z]\", \" \", newString) #remove digits and special characters\n",
    "            newString = re.sub('[m]{2,}', 'mm', newString) #punctuation\n",
    "            cleaned_post = newString\n",
    "            return cleaned_post\n",
    "\n",
    "        t = StringType()\n",
    "        out_col = self.getOutputCol()\n",
    "        in_col = dataset[self.getInputCol()]\n",
    "        return dataset.withColumn(out_col, udf(f, t)(in_col))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english')) \n",
    "\n",
    "class RemoveStopWords(Transformer, HasInputCol, HasOutputCol):\n",
    "\n",
    "    @keyword_only\n",
    "    def __init__(self, inputCol=None, outputCol=None):\n",
    "        super(RemoveStopWords, self).__init__()\n",
    "        kwargs = self._input_kwargs\n",
    "        self.setParams(**kwargs)\n",
    "\n",
    "    @keyword_only\n",
    "    def setParams(self, inputCol=None, outputCol=None):\n",
    "        kwargs = self._input_kwargs\n",
    "        return self._set(**kwargs)\n",
    "\n",
    "    def _transform(self, dataset):\n",
    "\n",
    "        def f(s):\n",
    "            newString = s\n",
    "            tokens = [w for w in newString.split() if not w in stop_words] #stopwords\n",
    "            words=[]\n",
    "            for i in tokens:\n",
    "                words.append(i)   \n",
    "            cleaned_post = (\" \".join(words)).strip()\n",
    "            return cleaned_post\n",
    "\n",
    "        t = StringType()\n",
    "        out_col = self.getOutputCol()\n",
    "        in_col = dataset[self.getInputCol()]\n",
    "        return dataset.withColumn(out_col, udf(f, t)(in_col))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RemoveShortWords(Transformer, HasInputCol, HasOutputCol, HasRegParam):\n",
    "\n",
    "    @keyword_only\n",
    "    def __init__(self, inputCol=None, outputCol=None, regParam=None):\n",
    "        super(RemoveShortWords, self).__init__()\n",
    "        kwargs = self._input_kwargs\n",
    "        self.setParams(**kwargs)\n",
    "\n",
    "    @keyword_only\n",
    "    def setParams(self, inputCol=None, outputCol=None, regParam=None):\n",
    "        kwargs = self._input_kwargs\n",
    "        return self._set(**kwargs)\n",
    "\n",
    "    def _transform(self, dataset):\n",
    "\n",
    "        def f(s):\n",
    "            newString = s\n",
    "            tokens = newString.split()\n",
    "            long_words=[]\n",
    "            tval = int(self.getRegParam())\n",
    "            for i in tokens:\n",
    "                if len(i)>tval:                                                 #removing short word\n",
    "                    long_words.append(i)   \n",
    "            cleaned_post = (\" \".join(long_words)).strip()\n",
    "            return cleaned_post\n",
    "\n",
    "        t = StringType()\n",
    "        out_col = self.getOutputCol()\n",
    "        in_col = dataset[self.getInputCol()]\n",
    "        return dataset.withColumn(out_col, udf(f, t)(in_col))\n",
    "    \n",
    "    def setThreshold(self, value):\n",
    "        return self._set(thresholdValue=value)\n",
    "\n",
    "    def getThreshold(self):\n",
    "        return self.getOrDefault(self.thresholdValue)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19.json\n",
      "8.json\n",
      "14.json\n",
      "7.json\n",
      "12.json\n",
      "2.json\n",
      "6.json\n",
      "17.json\n",
      "18.json\n",
      "10.json\n",
      "1.json\n",
      "5.json\n",
      "9.json\n",
      "16.json\n",
      "Data Ingestion Time:\n",
      "--- 25.797974586486816 seconds ---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "128873"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"title\", StringType(), True),\n",
    "    StructField(\"abstract\", StringType(), True)\n",
    "])\n",
    "total_df = spark.createDataFrame([], schema)\n",
    "\n",
    "ingestion_start_time = time.time()\n",
    "\n",
    "for file_name in os.listdir(\"/floyd/input/sdata250gb/\"):\n",
    "    if file_name != '._1.json':\n",
    "        print(file_name)\n",
    "        df = spark.read.json(\"/floyd/input/sdata250gb/\"+file_name)\n",
    "        dataNode = df.select(\"title\", \"abstract\")\n",
    "        total_df = total_df.union(dataNode)\n",
    "#for file_name in os.listdir(\"/floyd/input/sdata250gb1/\"):\n",
    "##    print(file_name)\n",
    " #   df = spark.read.json(\"/floyd/input/sdata250gb1/\"+file_name)\n",
    " #   dataNode = df.select(\"title\", \"abstract\")\n",
    " #   total_df = total_df.union(dataNode)\n",
    "#for file_name in os.listdir(\"/floyd/input/sdata250gb3/\"):\n",
    "#    print(file_name)\n",
    "#    df = spark.read.json(\"/floyd/input/sdata250gb3/\"+file_name)\n",
    "#    dataNode = df.select(\"title\", \"abstract\")\n",
    "#    total_df = total_df.union(dataNode)\n",
    "#for file_name in os.listdir(\"/floyd/input/sdata250gb4/\"):\n",
    "#    print(file_name)\n",
    "#    df = spark.read.json(\"/floyd/input/sdata250gb4/\"+file_name)\n",
    "#    dataNode = df.select(\"title\", \"abstract\")\n",
    "#    total_df = total_df.union(dataNode)\n",
    "#for file_name in os.listdir(\"/floyd/input/sdata250gb5/\"):\n",
    "#    if file_name != '._65.json':\n",
    "#        print(file_name)\n",
    "#       dataNode = df.select(\"title\", \"abstract\")\n",
    "#        total_df = total_df.union(dataNode)\n",
    "\n",
    "\n",
    "ingestion_time = time.time() - ingestion_start_time\n",
    "print(\"Data Ingestion Time:\")\n",
    "print(\"--- %s seconds ---\" % ingestion_time)\n",
    "\n",
    "total_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-Cleaning Time : \n",
      "\n",
      "--- 0.010288238525390625 seconds ---\n",
      "Cleaning Time:\n",
      "--- 0.10741662979125977 seconds ---\n",
      "Post-Cleaning Time : \n",
      "\n",
      "--- 162.54743146896362 seconds ---\n",
      "Total Preprocessing Time: \n",
      "\n",
      "--- 162.66801238059998 seconds ---\n"
     ]
    }
   ],
   "source": [
    "preprocess_start_time = time.time()\n",
    "\n",
    "#remove null values\n",
    "total_df=total_df.filter(total_df.title.isNotNull())\n",
    "total_df=total_df.filter(total_df.abstract.isNotNull())\n",
    "\n",
    "#remove duplicates\n",
    "total_df = total_df.dropDuplicates(['abstract'])\n",
    "preclean_time = time.time() - preprocess_start_time\n",
    "print(\"Pre-Cleaning Time : \\n\")\n",
    "print(\"--- %s seconds ---\" % preclean_time)\n",
    "\n",
    "convert_to_lower_title = ConvertToLower(inputCol=\"title\", outputCol=\"lower_title\")\n",
    "convert_remove_html_tags = RemoveHTMLTags(inputCol=convert_to_lower_title.getOutputCol(), outputCol=\"text_title\")\n",
    "convert_remove_unwanted_characters = RemoveUnwantedCharacters(inputCol=convert_remove_html_tags.getOutputCol(), outputCol=\"cleaned_title\")\n",
    "clean_text_title = RemoveShortWords(inputCol=convert_remove_unwanted_characters.getOutputCol(), outputCol=\"final_title\", regParam=1)\n",
    "pipeline = Pipeline(stages=[\n",
    "    convert_to_lower_title,\n",
    "    convert_remove_html_tags,\n",
    "    convert_remove_unwanted_characters,\n",
    "    clean_text_title\n",
    "    ])\n",
    "convert_to_lower_abstract = ConvertToLower(inputCol=\"abstract\", outputCol=\"lower_abstract\")\n",
    "convert_remove_html_tags_abstract = RemoveHTMLTags(inputCol=convert_to_lower_abstract.getOutputCol(), outputCol=\"text_abstract\")\n",
    "convert_remove_unwanted_characters_abstract = RemoveUnwantedCharacters(inputCol=convert_remove_html_tags_abstract.getOutputCol(), outputCol=\"cleaned_abstract\")\n",
    "convert_remove_stop_words = RemoveStopWords(inputCol=convert_remove_unwanted_characters_abstract.getOutputCol(), outputCol=\"wo_stopwords_abstract\")\n",
    "clean_text_abstract = RemoveShortWords(inputCol=convert_remove_stop_words.getOutputCol(), outputCol=\"final_abstract\", regParam=1)\n",
    "pipeline_abstract = Pipeline(stages=[\n",
    "    convert_to_lower_abstract,\n",
    "    convert_remove_html_tags_abstract,\n",
    "    convert_remove_unwanted_characters_abstract,\n",
    "    convert_remove_stop_words,\n",
    "    clean_text_abstract\n",
    "    ])\n",
    "\n",
    "model = pipeline.fit(total_df)\n",
    "model_abstract = pipeline_abstract.fit(total_df)\n",
    "\n",
    "clean_start_time = time.time()\n",
    "cleaned_title = model.transform(total_df)\n",
    "cleaned_abstract = model_abstract.transform(total_df)\n",
    "clean_time = time.time() - clean_start_time\n",
    "print(\"Cleaning Time:\")\n",
    "print(\"--- %s seconds ---\" % clean_time)\n",
    "\n",
    "postclean_start_time = time.time()\n",
    "\n",
    "cleaned_title = cleaned_title.select(\"title\", \"abstract\", \"final_title\").toPandas()\n",
    "cleaned_abstract = cleaned_abstract.select(\"title\", \"abstract\", \"final_abstract\").toPandas()\n",
    "data = pd.merge(cleaned_title, cleaned_abstract, on=['title','abstract'])\n",
    "#data = data.drop(['title', 'abstract'], axis=1)\n",
    "\n",
    "data = data.rename(columns={\"title\": \"summary\", \"abstract\": \"text\"})\n",
    "data = data.rename(columns={\"final_title\": \"cleaned_summary\", \"final_abstract\": \"cleaned_text\"})\n",
    "\n",
    "data.replace('', np.nan, inplace=True)\n",
    "data.dropna(axis=0,inplace=True)\n",
    "\n",
    "postclean_time = time.time() - postclean_start_time\n",
    "print(\"Post-Cleaning Time : \\n\")\n",
    "print(\"--- %s seconds ---\" % postclean_time)\n",
    "total_preprocess_time = time.time() - preprocess_start_time\n",
    "print(\"Total Preprocessing Time: \\n\")\n",
    "print(\"--- %s seconds ---\" % total_preprocess_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "summary            88709\n",
       "text               88709\n",
       "cleaned_summary    88709\n",
       "cleaned_text       88709\n",
       "dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data = data.dropna()\n",
    "#datay = data.sort_values(by='summary', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#datay.to_csv('withspark_1.csv')\n",
    "#datax = pd.read_csv(\"withoutspark.csv\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "datay['cleaned_summary'].isin(datax['cleaned_summary']).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY8AAAEICAYAAACnL3iHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAIABJREFUeJzt3X+UVeV97/H3JxCt1Sig6ZSC6ZBK7UJtVOYqXclNpxoRMQ32LpNivQENNyRXbWPjasQ2a2n90YvtNVaNMSWBCpYELWqgEUOoelbbdQOKPyLijzIilmEhREHIaDTBfO8f+zm4PXNm5uzhzMw5w+e11lln7+9+9j7P3vM88z177+eco4jAzMysiPcNdQXMzKz5OHmYmVlhTh5mZlaYk4eZmRXm5GFmZoU5eZiZWWFOHmZmVpiTh5k1NElbJH2iDtu5U9L19aiTOXlYjSSNHOo6mFnjcPIYRJKulLRN0k8lvSDpzMp3Q5LaJXXm5rdI+gtJT0t6Q9JCSS2SHkzb+VdJo1PZVkkh6WJJWyXtlvRFSf8trf+6pK/ntv1bkh6W9JqkVyUtlTSq4rWvlPQ08Eaqx70V+3SrpFsG9MDZQUvSXcCHgH+R1CXpK5KmSPp/qT3/WFJ7KjtGUqekP0zzR0jqkDRL0lzgQuAraTv/MmQ7NVxEhB+D8ACOB7YCv5HmW4HfAu4Ers+Vawc6c/NbgLVACzAO2Ak8AZwC/ArwMHB1bpsBfDMtmwq8BXwP+LXc+r+fyh8HnAUcCnwQ+Dfg7yte+yngWOAwYCzwBjAqLR+Ztjd5qI+vH8P3kdrhJ9L0OOA1YDrZm9+z0vwH0/KpwCupvX8LWJ7bznv6mh8H9vCZx+B5h+yf9CRJ74+ILRHxYo3r3hYROyJiG/DvwLqIeDIi3gLuJ0skeddFxFsR8UOyf/bfjYidufVPAYiIjohYExFvR8RPgK8Bv1+xrVsjYmtE/CwitpMlmE+nZdOAVyPi8UJHwqz//iewKiJWRcQvI2INsJ4smZDa/D8DD6XYF4aspsOck8cgiYgO4HLgGmCnpGWSfqPG1Xfkpn9WZf6I/pRPl7+WpUtpe4F/Ao6p2NbWivnFZB2Y9HxXjftgVg+/CXw6XbJ6XdLrwMfIzorLFgAnAndGxGtDUcmDgZPHIIqI70TEx8g6QAA3kp0Z/Gqu2K8PYpX+JtXjpIg4kiwZqKJM5dcufw/4XUknAp8Elg54Le1gl2+DW4G7ImJU7nF4RMwHkDSCLHksAS6RdFwP27ED5OQxSCQdL+kMSYeS3Yf4GfBLsnsK09PNvl8nOzsZLB8AuoA9ksYBf9HXCulS2XLgO8CjEfFfA1tFM3YAH07T/wT8oaSzJY2Q9CtpkMn4tPwvyZLE54C/A5akhFK5HTtATh6D51BgPvAq797Qu4rsss+PyW4K/hC4exDr9NfAqcAe4AHgvhrXWwychC9Z2eD4P8BX0yWqPwZmkCWJn5CdifwF8D5Jk4EvA7Mi4h2yM/sA5qXtLCS75/i6pO8N8j4MO0qjEMxqJulDwPPAr0fE3qGuj5kNPp95WCGS3kf27m6ZE4fZwcufGraaSTqc7Lrxy2TDdM3sIOXLVmZmVpgvW5mZWWFNe9nqmGOOidbW1v3zb7zxBocffvjQVaiAZqorNFd9i9T18ccf3wU8TfbVLwEsiIhbJI0hG/XWSjYK7jMRsVuSgFvIPrn8JnBRRDwBIGk28NW06esjYnGKTyb7WozDgFXAlyIienqN3upb2eb7s88HEx+X7h5//PFXI+KDddlYLd9hAvw5sBF4Bvgu2fcmTQDWAR1kneCQVPbQNN+RlrfmtnNVir8AnJ2LT0uxDmBeLXWaPHly5D3yyCPRLJqprhHNVd8idSUbIn1qNskHgP8EJgF/W26HZMM8b0zT04EHyT5IOYXsa2IAxgCb0/PoND06LXs0lVVa95wUr/oavT0q23x/9vlg4uPSHbA+Buu7rdKHx/4MaIuIE4ERwEyyMdQ3R8RxwG5gTlplDrA7xW9O5ZA0Ka13QkoW30gf8hkB3A6ckzruBams2UD7RaQzh4j4KfAc2RfvzSD7LAvp+bw0PQNYkvrhWmCUpLHA2cCaiNgV2dnDGmBaWnZkRKxNHXdJxbaqvYZZU6j1stVI4DBJvyD7Ko3twBnAn6Tli8m+s+kOsk5xTYovB76eTvdnkA3vfBt4SVIHcFoq1xERmwEkLUtln+3/bpkVI6mV7Asj1wEtkX0JJGQf6GxJ0+N473d9daZYb/HOKnF6eY3Kes0F5gK0tLRQKpW6lenq6qoaP9j5uAysPpNHRGyT9H+B/yL7So0fAo8Dr0fEvlQs3yn2d6SI2CdpD3B0iq/NbTq/TmXHO71fe2PWD5KOAO4FLo+Ivdl7nUxEhKQBHZLY22tExAKy72qira0t2tvbu5UplUpUix/sfFwGVp/JI/3Q0Ayyexyvk33d8ZCM8e/tXVgzvctoprpCc9W3aF0lvZ8scSyNiPLXs+yQNDYitqdLTztTfBvZb5uUjU+xbWS/w5KPl1J8fJXyvb2GWVOo5bLVJ4CXIvu9ByTdB3yU7HrvyHT2ke8U5Q7WmX669CiyH2vpqePRS/w9ensX1kzvMpqprtBc9e1HXRcCz0XE13KxlcBssu8imw2syMUvS5dWTwf2pH/+q4G/Kf+iI9kPEl0VEbsk7ZU0hexy2Czgtj5ew6wp1PI5j/8Cpkj61XTv4kyy+xGPAOenMpUdbHaaPh94ON0sXAnMlHSopAnARLKRKI8BEyVNkHQI2U31lQe+a2Z9OgL4LHCGpKfSYzrZP/SzJG0ie/M0P5VfRTaSqoPsV+ouAYiIXcB1ZG35MeDaFCOV+XZa50WyEVf08hpmTaGWex7rJC0n++nTfcCTZO/+HwCWKfv97SfJ3sGRnu9KN8R3kSUDImKjpHvIEs8+4NLIvvkSSZcBq8lGci2KiI3120WzHnVFROXvl5SdWRlIb4IurVY4IhYBi6rE15P9MFFl/LVqr2HWLGoabRURVwNXV4Q38+5oqXzZt3j3Z0orl90A3FAlvorsXZ2ZmTUBfz2JmZkV1rRfTzLQWuc98J75LfPPHaKamBXn9msDzWceZmZWmJOHmZkV5uRhZmaFOXmYmVlhTh5mZlaYk4eZmRXm5GFmZoU5eZiZWWFOHmZmVpg/YU73T+OamVnvfOZhZmaFOXmYmVlhTh5mZlaYk4eZmRXm5GFmZoU5edjBrFXSTknPlAOS7s79nvkWSU+leKukn+WWfTO3zmRJGyR1SLpVklJ8jKQ1kjal59EprlSuQ9LTkk4d7B03O1B9Jg9Jx+c6zFOS9kq6vD8dQ9LsVH6TpNm5eNXOZzbAXgWm5QMR8ccRcXJEnAzcC9yXW/xieVlEfDEXvwP4PDAxPcrbnAc8FBETgYfSPMA5ubJz0/pmTaXP5BERL+Q602TgTeB+CnYMSWPIfgf9dLLfPr+6nHDoufOZDaQuYFe1BekNzGeA7/a2AUljgSMjYm1EBLAEOC8tngEsTtOLK+JLIrMWGJW2Y9Y0in5I8Eyyd18vS5oBtKf4YqAEXEmuYwBrJZU7RjuwJiJ2AUhaA0yTVCJ1vhQvd74HD2C/zA7Ufwd2RMSmXGyCpCeBvcBXI+LfgXFAZ65MZ4oBtETE9jT9CtCSpscBW6uss50KkuaSvQmjpaWFUqnUraJdXV3d4lectO8989XWG+6qHRern6LJYybvvhMr2jF6i/fU+d6jt450IA2lsqNVU89G2GyNupnqW8e6XsB7zzq2Ax+KiNckTQa+J+mEWjcWESEpilYiIhYACwDa2tqivb29W5lSqURl/KLK3zC/sPt6w12142L1U3PykHQI8Cngqspl/e0YRfXWkQ6koVR2tGrq2fmarVE3U33rUVdJI4H/QXaZFoCIeBt4O00/LulF4LeBbcD43OrjUwxgh6SxEbE9nX3vTPFtwLE9rGPWFIqMtjoHeCIidqT5HeXrtDV2jN7iPXU+s6HwCeD5iNh/Rizpg5JGpOkPk92b25zOvvdKmpLuk8wCVqTVVgLlgSGzK+Kz0uCSKcCe3Fm8WVMokjwqT+OLdozVwFRJo9ON8qnA6j46n9lAmgD8CDheUqekOSmevzxb9nHg6TR0dznwxfL9O+AS4NtAB/Ai796vmw+cJWkTWUKan+KrgM2p/LfS+mZNpabLVpIOB84CvpALzwfuSR3uZbKRKZB1jOlkHeNN4GKAiNgl6TrgsVTu2orOdydwGFnH881yGwwvRURbZTAiLqoSu5ds6G43EbEeOLFK/DWyQSaV8QAu7Ud9zRpGTckjIt4Ajq6IFe4YEbEIWFQlXrXzNZJqX9u+Zf65Q1ATM7Oh50+Ym5lZYU4eZmZWmJOHmZkV5uRhZmaFOXmYmVlhTh5mZlaYk4eZmRXm5GFmZoU5eZiZWWFOHmZmVpiTh5mZFebkYWZmhTl5mJlZYU4eZmZWmJOHmZkV5uRhZmaFOXmYmVlhTh52MGuVtFPSM+WApGskbZP0VHpMzy27SlKHpBcknZ2LT0uxDknzcvEJktal+N2SDknxQ9N8R1reOji7a1Y/NSUPSaMkLZf0vKTnJP2epDGS1kjalJ5Hp7KSdGvqGE9LOjW3ndmp/CZJs3PxyZI2pHVulaT676pZN68C06rEb46Ik9NjFYCkScBM4IS0zjckjZA0ArgdOAeYBFyQygLcmLZ1HLAbmJPic4DdKX5zKmfWVGo987gF+EFE/A7wEeA5YB7wUERMBB5K85B1oonpMRe4A0DSGOBq4HTgNODqcsJJZT6fW69ahzarty5gV41lZwDLIuLtiHgJ6CBrx6cBHRGxOSJ+DiwDZqQ3QGcAy9P6i4HzcttanKaXA2f6DZM1m5F9FZB0FPBx4CKA1EF+LmkG0J6KLQZKwJVkHWNJRASwNp21jE1l10TErrTdNcA0SSXgyIhYm+JLyDrZg3XZQ7PiLpM0C1gPXBERu4FxwNpcmc4UA9haET8dOBp4PSL2VSk/rrxOROyTtCeVf7WyIpLmkr0Jo6WlhVKp1K2yXV1d3eJXnLTvPfPV1hvuqh0Xq58+kwcwAfgJ8I+SPgI8DnwJaImI7anMK0BLmt7fMZJyp+kt3lkl3k1vHelAGkplR6tVf1+v2Rp1M9W3DnW9A7gOiPR8E/C5A69Z/0TEAmABQFtbW7S3t3crUyqVqIxfNO+B98xvubD7esNdteNi9VNL8hgJnAr8aUSsk3QL716iAiAiQlIMRAUrXqfHjnQgDaWyo9Wqvx2y2Rp1M9X3QOsaETvK05K+BXw/zW4Djs0VHZ9i9BB/DRglaWQ6+8iXL2+rU9JI4KhU3qxp1HLPoxPojIh1aX45WTLZkS5HkZ53puU9dbLe4uOrxM0GXblNJ38ElEdirQRmppFSE8juzT0KPAZMTCOrDiG7qb4yXbZ9BDg/rT8bWJHbVnnAyPnAw6m8WdPo88wjIl6RtFXS8RHxAnAm8Gx6zAbm071jXCZpGdm13z0RsV3SauBvcjfJpwJXRcQuSXslTQHWAbOA2+q4j9209vNMw4adCcCPgGMkdZIN6GiXdDLZZastwBcAImKjpHvI2v0+4NKIeAdA0mXAamAEsCgiNqbtXwksk3Q98CSwMMUXAndJ6iC7YT/zQHZiw7Y9/T57NuuvWi5bAfwpsDS9s9oMXEx21nKPpDnAy8BnUtlVwHSy0ShvprKkJHEd2Ts1gGvLN8+BS4A7gcPIbpT7ZrkNhpcioq0itrBqSSAibgBuqBJfRdbuK+ObyUZjVcbfAj5duLZmDaSm5BERTwGVnQyys5DKsgFc2sN2FgGLqsTXAyfWUhczMxt6/oS5mZkV5uRhZmaFOXmYmVlhTh5mZlaYk4eZmRXm5GFmZoU5eZiZWWFOHmZmVpiTh5mZFebkYWZmhTl5mJlZYU4eZmZWmJOHmZkVVutXslsVlb8LsmX+uUNUEzOzweUzDzMzK8zJw8zMCnPyMDOzwmpKHpK2SNog6SlJ61NsjKQ1kjal59EpLkm3SuqQ9LSkU3PbmZ3Kb5I0OxefnLbfkdZVvXfUrIpWSTslPVMOSPo7Sc+ntnu/pFEp3irpZ6kPPCXpm7l1qrbf/vQRs2ZR5MzjDyLi5NxvPs8DHoqIicBDaR7gHGBieswF7oCsIwFXA6eT/a7z1eXOlMp8PrfetH7vkVntXqV7W1sDnBgRvwv8J3BVbtmLqQ+cHBFfzMV7ar+F+ohZMzmQy1YzgMVpejFwXi6+JDJrgVGSxgJnA2siYldE7CbrpNPSsiMjYm36/fMluW2ZDaQuYFc+EBE/jIh9aXYtML63DfTRfov2EbOmUetQ3QB+KCmAf4iIBUBLRGxPy18BWtL0OGBrbt3OFOst3lklbjbUPgfcnZufIOlJYC/w1Yj4d3pvv0X7yHYqSJpLdnZCS0sLpVKpWyVbDoMrTtrXLZ5Xbb3hrqur66Dc78FSa/L4WERsk/RrwBpJz+cXRkSkxDKgeutIRRpKXx2tv2p9/WZr1M1U33rVVdJfAfuApSm0HfhQRLwmaTLwPUkn1Lq9/vaR9EZtAUBbW1u0t7d3K3Pb0hXctKH3rrzlwu7rDXelUolqx8vqo6bkERHb0vNOSfeT3bPYIWlsRGxPp9w7U/FtwLG51cen2DagvSJeSvHxVcpXq0ePHalIQ7mo4sN99VJrB222Rt1M9a1HXSVdBHwSODNdiiIi3gbeTtOPS3oR+G16b79F+4hZ0+jznoekwyV9oDwNTAWeAVYC5RFTs4EVaXolMCuNKJkC7Emn7quBqZJGpxvlU4HVadleSVPSKJVZuW2ZDSpJ04CvAJ+KiDdz8Q9KGpGmP0x2s3tzH+23aB8xaxq1nHm0APen0Ycjge9ExA8kPQbcI2kO8DLwmVR+FTAd6ADeBC4GiIhdkq4DHkvlro2I8s3KS4A7gcOAB9PDbKBNAH4EHCOpk2w04FXAoWSXZwHWppFVHweulfQL4JfAF2tov/Mp0EfMmkmfySMiNgMfqRJ/DTizSjyAS3vY1iJgUZX4euDEGuprVk8v5Yaely2sVjAi7gXu7WFZ1fbbnz5i1iz8CXMzMyvMycPMzApz8jAzs8KcPMzMrDAnDzMzK8zJw8zMCnPyMDOzwpw8zMysMCcPMzMrzMnDzMwKc/IwM7PCnDzMzKwwJw8zMyvMycPMzApz8jAzs8KcPMzMrDAnDzMzK8zJww5mrZJ2SnqmHJA0RtIaSZvS8+gUl6RbJXVIelrSqbl1ZqfymyTNzsUnS9qQ1rk1/cZ5j69h1kxqTh6SRkh6UtL30/wESetSx7hb0iEpfmia70jLW3PbuCrFX5B0di4+LcU6JM2r3+6Z9epVYFpFbB7wUERMBB5K8wDnABPTYy5wB2SJgOy3z08HTgOuziWDO4DP59ab1sdrmDWNImceXwKey83fCNwcEccBu4E5KT4H2J3iN6dySJoEzAROIOtE30gJaQRwO1nnnARckMqaDbQuYFdFbAawOE0vBs7LxZdEZi0wStJY4GxgTUTsiojdwBpgWlp2ZESsTb9ZvqRiW9Vew6xpjKylkKTxwLnADcCX0+n3GcCfpCKLgWvI3mnNSNMAy4Gvp/IzgGUR8TbwkqQOsndqAB0RsTm91rJU9tkD2jOz/mmJiO1p+hWgJU2PA7bmynWmWG/xzirx3l5jwLTOe6BbbMv8cwf6ZW0Yqyl5AH8PfAX4QJo/Gng9Ival+XzH2N+ZImKfpD2p/DhgbW6b+XUqO9/p1SohaS7ZJQNaWloolUr7l3V1db1nvjdXnLSv70L9UOvrF6lrI2im+tazrhERkqIuG+vna/TW5staDutfm26Wv2l/NVO7bUZ9Jg9JnwR2RsTjktoHvko9i4gFwAKAtra2aG9/tzqlUon8fG8uqvIurB62XFjb6xepayNopvrWoa47JI2NiO3p0tPOFN8GHJsrNz7FtgHtFfFSio+vUr631+imtzZfdtvSFdy0odb3ge+qtb02q2Zqt82olnseHwU+JWkLsIzsctUtZNd8yy023zH2d7K0/CjgNXrvfNXiZkNhJVAeMTUbWJGLz0qjrqYAe9Klp9XAVEmj043yqcDqtGyvpCnpsu2sim1Vew2zptFn8oiIqyJifES0kt3wfjgiLgQeAc5PxSo7WbljnJ/KR4rPTKOxJpCNPnkUeAyYmEZvHZJeY2Vd9s6sdxOAHwHHS+qUNAeYD5wlaRPwiTQPsArYDHQA3wIuAYiIXcB1ZO34MeDaFCOV+XZa50XgwRTv6TXMmkbxc913XQksk3Q98CSwMMUXAnelG+K7yJIBEbFR0j1kN8L3AZdGxDsAki4jewc3AlgUERsPoF5mtXopItqqxM+sDKQ3QJdW20hELAIWVYmvB06sEn+t2muYNZNCySMiSmTXc0mjo06rUuYt4NM9rH8D2Yityvgqsnd2A6LaSBMzM+s/f8LczMwKc/IwM7PCnDzMzKwwJw8zMyvMycPMzAo7kKG6VqFyVJe/O8jMhiufeZiZWWFOHmZmVpiTh5mZFebkYWZmhTl5mJlZYU4eZmZWmJOHmZkV5uRhZmaFOXmYmVlhTh5mZlaYk4eZmRXm5GFWQdLxkp7KPfZKulzSNZK25eLTc+tcJalD0guSzs7Fp6VYh6R5ufgESetS/G5Jhwz2fpodiD6Th6RfkfSopB9L2ijpr1O8auOXdGia70jLW3PbKtTBzIZCRLwQESdHxMnAZOBN4P60+ObysvTzyUiaBMwETgCmAd+QNELSCOB24BxgEnBBKgtwY9rWccBuYM5g7Z9ZPdRy5vE2cEZEfAQ4GZgmaQo9N/45wO4UvzmV628HMxtqZwIvRsTLvZSZASyLiLcj4iWgAzgtPToiYnNE/BxYBsyQJOAMYHlafzFw3oDtgdkA6PMr2SMigK40+/70CLLG/ycpvhi4BriDrCNdk+LLga+nzrK/gwEvSSp3MEgdDEDSslT22QPZMbM6mQl8Nzd/maRZwHrgiojYDYwD1ubKdKYYwNaK+OnA0cDrEbGvSvn3kDQXmAvQ0tJCqVTqVqblMLjipH3d4n2ptq3hpKura9jv41Cq6fc80tnB48BxZGcJL9Jz4x9H6jARsU/SHrLOUrSDVatHjx2pt4bSn45VDz3Vp9kadTPVt551TZdiPwVclUJ3ANeRvXm6DrgJ+FxdXqwHEbEAWADQ1tYW7e3t3crctnQFN20o/tM8Wy7svq3hpFQqUe14WX3U1OIi4h3gZEmjyK79/s6A1qrnevTYkXprKBdV/EjTYOmpczZbo26m+ta5rucAT0TEDoDyM4CkbwHfT7PbgGNz641PMXqIvwaMkjQyvQHLlzdrCoVGW0XE68AjwO+RGn9alG/8+ztSWn4UWWfpqYP11vHMhtIF5C5ZSRqbW/ZHwDNpeiUwMw0WmQBMBB4FHgMmpsElh5BdAluZLgU/Apyf1p8NrBjQPTGrs1pGW30wnXEg6TDgLOA5em78K9M8afnDqbMU6mD12Dmz/pJ0OFlbvy8X/ltJGyQ9DfwB8OcAEbERuIfsPt0PgEsj4p10VnEZsJqsz9yTygJcCXw53fs7Glg4CLtlVje1XLYaCyxO9z3eR9YBvi/pWWCZpOuBJ3m38S8E7kqdYhdZMiAiNkoqd7B9pA4GIKncwUYAi3IdzGxIRMQbZP/U87HP9lL+BuCGKvFVwKoq8c28O2DErOnUMtrqaeCUKvGqjT8i3gI+3cO2CnUwMzNrTP6EuZmZFebkYWZmhTl5mJlZYU4eZmZWmJOHmZkV5uRhZmaFOXmYmVlhTh5mZlaYk4eZmRXm5GFmZoU5eZiZWWFOHmZmVljxnx9rcK1D9MNPZmYHk2GXPBpJtUS2Zf65Q1ATM7P68mUrMzMrzMnDzMwKc/IwM7PCavkN82MlPSLpWUkbJX0pxcdIWiNpU3oeneKSdKukDklPSzo1t63ZqfwmSbNz8cnpt6E70roaiJ01q5WkLalNPiVpfYq5zZsltZx57AOuiIhJwBTgUkmTgHnAQxExEXgozQOcA0xMj7nAHZB1POBq4HSyn6+9utz5UpnP59abduC7ZnbA/iAiTo6ItjTvNm+W9Jk8ImJ7RDyRpn8KPAeMA2YAi1OxxcB5aXoGsCQya4FRksYCZwNrImJXROwG1gDT0rIjI2JtRASwJLcts0biNm+WFBqqK6kVOAVYB7RExPa06BWgJU2PA7bmVutMsd7inVXi1V5/Ltk7O1paWiiVSvuXdXV1USqVuOKkfUV2adCVSqX9dW0WzVTfOtY1gB9KCuAfImIBDdbmy1oOo1/tvln+pv3VTO22GdWcPCQdAdwLXB4Re/OXaCMiUicbUKkDLwBoa2uL9vb2/ctKpRLt7e1c1OAfEtxyYfv+ujaLZqpvHev6sYjYJunXgDWSns8vbIQ2X3bb0hXctKH4R7a2XNh9W8NJM7XbZlTTaCtJ7ydLHEsj4r4U3pFOv0nPO1N8G3BsbvXxKdZbfHyVuNmQiYht6XkncD/ZPQu3ebOkltFWAhYCz0XE13KLVgLl0SOzgRW5+Kw0AmUKsCed6q8GpkoanW4aTgVWp2V7JU1JrzUrty2zQSfpcEkfKE+TtdVncJs326+Wc92PAp8FNkh6KsX+EpgP3CNpDvAy8Jm0bBUwHegA3gQuBoiIXZKuAx5L5a6NiF1p+hLgTuAw4MH0MBsqLcD96dLsSOA7EfEDSY/hNm8G1JA8IuI/gJ7GoJ9ZpXwAl/awrUXAoirx9cCJfdXFbDBExGbgI1Xir+E2bwb4E+ZmZtYPTh5mZlaYk4eZmRXm5GFmZoU5eZiZWWFOHmZmVpiTh5mZFebkYWZmhTl5mJlZYU4eZmZWmJOHmZkV5uRhZmaFOXkMstZ5D7Bh2x5a5z1Aa4P/cJWZWU+cPMzMrDAnDzMzK8zJw8zMCnPyMDOzwpw8zMyssD6Th6RFknZKeiYXGyNpjaRN6Xl0ikvSrZI6JD0t6dTcOrNT+U2SZufikyVtSOvcqvTD0WZDRdKxkh6R9KykjZK+lOLXSNom6an0mJ5b56rUhl+QdHYuPi3FOiTNy8UnSFqX4ndLOmRw99LswNRy5nEnMK0iNg94KCImAg+leYBzgInpMRe4A7JkA1x1omKlAAAIaklEQVQNnA6cBlxdTjipzOdz61W+ltlg2wdcERGTgCnApZImpWU3R8TJ6bEKIC2bCZxA1n6/IWmEpBHA7WT9YhJwQW47N6ZtHQfsBuYM1s6Z1UOfySMi/g3YVRGeASxO04uB83LxJZFZC4ySNBY4G1gTEbsiYjewBpiWlh0ZEWsjIoAluW2ZDYmI2B4RT6TpnwLPAeN6WWUGsCwi3o6Il4AOsjdJpwEdEbE5In4OLANmpLPrM4Dlaf18HzJrCiP7uV5LRGxP068ALWl6HLA1V64zxXqLd1aJVyVpLtkZDS0tLZRKpf3Lurq6KJVKXHHSvn7szuBqOYz99czvQ6MqH9tmUO+6SmoFTgHWAR8FLpM0C1hPdnaym6zNrs2tlm/Hle3+dOBo4PWI2FelfOXr99jmy/LtqYjblq54z/xJ444qvI1G1kztthn1N3nsFxEhKepRmRpeawGwAKCtrS3a29v3LyuVSrS3t3NRE3xq+4qT9nHThuzQb7mwfWgrU4PysW0G9ayrpCOAe4HLI2KvpDuA64BIzzcBn6vLi/WgtzZfdtvSFfvb04FohrZYRDO122bU39FWO9IlJ9LzzhTfBhybKzc+xXqLj68SNxtSkt5PljiWRsR9ABGxIyLeiYhfAt8iuywFxdv9a2SXdEdWxM2aRn+Tx0qgPGJqNrAiF5+VRl1NAfaky1urgamSRqcb5VOB1WnZXklT0nXgWbltmQ2J1BYXAs9FxNdy8bG5Yn8ElEcgrgRmSjpU0gSygR+PAo8BE9PIqkPIbqqvTPf3HgHOT+vn+5BZU+jzXFfSd4F24BhJnWSjpuYD90iaA7wMfCYVXwVMJ7th+CZwMUBE7JJ0HVlnArg2Iso34S8hG9F1GPBgepgNpY8CnwU2SHoqxf6SbLTUyWSXrbYAXwCIiI2S7gGeJRupdWlEvAMg6TKyN08jgEURsTFt70pgmaTrgSfJkpVZ0+gzeUTEBT0sOrNK2QAu7WE7i4BFVeLrgRP7qofZYImI/wCqfd5oVS/r3ADcUCW+qtp6EbGZdy97mTUdf8LczMwKO/AhGnZAqv2mx5b55w5BTczMauczDzMzK8zJw8zMCnPyMDOzwpw8zMysMCcPMzMrzMnDzMwKc/IwM7PCnDzMzKwwf0iwAVV+cNAfGjSzRuMzDzMzK8xnHmYG+IzXivGZh5mZFebkYWZmhfmyVRPwN++aWaPxmYeZmRXmMw8zq8pnvNabhkkekqYBt5D91vO3I2L+EFepoXlkTPNzm7dm1hDJQ9II4HbgLKATeEzSyoh4dmhr1jycTJpLs7Z5tzMra4jkAZwGdETEZgBJy4AZQEN3pEZW7ZJDLfzPYNAMizbvdnbwapTkMQ7YmpvvBE6vLCRpLjA3zXZJeiG3+Bjg1QGrYR39WQPXVTdWDTdsfasoUtffHMiK9KEebb6smf4+QI/trN6a7rgMguPrtaFGSR41iYgFwIJqyyStj4i2Qa5SvzRTXaG56ttMda1Fb22+bLjtc734uHQnaX29ttUoQ3W3Acfm5senmNlw5TZvTa1RksdjwERJEyQdAswEVg5xncwGktu8NbWGuGwVEfskXQasJhu2uCgiNhbcTK+n9g2mmeoKzVXfpqhrndp8WVPs8xDwcemubsdEEVGvbZmZ2UGiUS5bmZlZE3HyMDOzwoZF8pA0TdILkjokzRvq+uRJOlbSI5KelbRR0pdSfIykNZI2pefRQ13XMkkjJD0p6ftpfoKkden43p1u8DYESaMkLZf0vKTnJP1eIx/bemvktl9vkhZJ2inpmVys6t9amVvTcXla0qm5dWan8pskzR6KfamXov9f6npcIqKpH2Q3G18EPgwcAvwYmDTU9crVbyxwapr+APCfwCTgb4F5KT4PuHGo65qr85eB7wDfT/P3ADPT9DeB/z3UdczVdTHwv9L0IcCoRj62dd73hm77A7C/HwdOBZ7Jxar+rYHpwIOAgCnAuhQfA2xOz6PT9Oih3rcDOCaF/r/U87gMhzOP/V/zEBE/B8pf89AQImJ7RDyRpn8KPEf26eIZZP/4SM/nDU0N30vSeOBc4NtpXsAZwPJUpJHqehTZP5SFABHx84h4nQY9tgOgodt+vUXEvwG7KsI9/a1nAEsisxYYJWkscDawJiJ2RcRuYA0wbeBrPzD68f+lbsdlOCSPal/zMG6I6tIrSa3AKcA6oCUitqdFrwAtQ1StSn8PfAX4ZZo/Gng9Ival+UY6vhOAnwD/mC6zfVvS4TTusa23pmn7A6inv3VPx2bYHrMa/7/U7bgMh+TRFCQdAdwLXB4Re/PLIjtvHPIx05I+CeyMiMeHui41Gkl2GeOOiDgFeIPsFH2/Rjm2NvAO5r/1UPx/GQ7Jo+G/5kHS+8n+sEsj4r4U3pFOF0nPO4eqfjkfBT4laQvZJZAzyH5vYpSk8gdKG+n4dgKdEbEuzS8nSyaNeGwHQsO3/UHQ09+6p2Mz7I5Zwf8vdTsuwyF5NPTXPKR7BguB5yLia7lFK4HyiIbZwIrBrluliLgqIsZHRCvZcXw4Ii4EHgHOT8Uaoq4AEfEKsFVS+ZtCzyT7SvOGO7YDpKHb/iDp6W+9EpiVRhdNAfakyzirgamSRqcRSFNTrCn14/9L/Y7LUI8WqNOIg+lkowxeBP5qqOtTUbePkZ0yPg08lR7Tye4lPARsAv4VGDPUda2odzvvjrb6MPAo0AH8M3DoUNcvV8+TgfXp+H6PbKRIQx/bOu9/w7b9AdjX7wLbgV+QnXXO6elvTTaa6PZ0XDYAbbntfC615Q7g4qHerwM8JoX+v9TzuPjrSczMrLDhcNnKzMwGmZOHmZkV5uRhZmaFOXmYmVlhTh5mZlaYk4eZmRXm5GFmZoX9f4MpSNAlqkAmAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "text_word_count = []\n",
    "summary_word_count = []\n",
    "\n",
    "# populate the lists with sentence lengths\n",
    "for i in data['cleaned_text']:\n",
    "      text_word_count.append(len(i.split()))\n",
    "\n",
    "for i in data['cleaned_summary']:\n",
    "      summary_word_count.append(len(i.split()))\n",
    "\n",
    "length_df = pd.DataFrame({'text':text_word_count, 'summary':summary_word_count})\n",
    "\n",
    "length_df.hist(bins = 30)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9545197352271829\n",
      "0.9265613233648281\n"
     ]
    }
   ],
   "source": [
    "cnt=0\n",
    "for i in data['cleaned_summary']:\n",
    "    if(len(i.split())<=20):\n",
    "        cnt=cnt+1\n",
    "print(cnt/len(data['cleaned_summary']))\n",
    "cnt=0\n",
    "for i in data['cleaned_text']:\n",
    "    if(len(i.split())<=200):\n",
    "        cnt=cnt+1\n",
    "print(cnt/len(data['cleaned_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_text_len=200\n",
    "max_summary_len=20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_text =np.array(data['cleaned_text'])\n",
    "cleaned_summary=np.array(data['cleaned_summary'])\n",
    "\n",
    "short_text=[]\n",
    "short_summary=[]\n",
    "\n",
    "for i in range(len(cleaned_text)):\n",
    "    if(len(cleaned_summary[i].split())<=max_summary_len and len(cleaned_text[i].split())<=max_text_len):\n",
    "        short_text.append(cleaned_text[i])\n",
    "        short_summary.append(cleaned_summary[i])\n",
    "        \n",
    "df=pd.DataFrame({'text':short_text,'summary':short_summary})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['summary'] = df['summary'].apply(lambda x : 'sostok '+ x + ' eostok')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_tr,x_val,y_tr,y_val=train_test_split(np.array(df['text']),np.array(df['summary']),test_size=0.1,random_state=0,shuffle=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer \n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "#prepare a tokenizer for reviews on training data\n",
    "x_tokenizer = Tokenizer() \n",
    "x_tokenizer.fit_on_texts(list(x_tr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "% of rare words in vocabulary: 64.30920149586076\n",
      "Total Coverage of rare words: 0.8832953040415562\n"
     ]
    }
   ],
   "source": [
    "thresh=4\n",
    "\n",
    "cnt=0\n",
    "tot_cnt=0\n",
    "freq=0\n",
    "tot_freq=0\n",
    "\n",
    "for key,value in x_tokenizer.word_counts.items():\n",
    "    tot_cnt=tot_cnt+1\n",
    "    tot_freq=tot_freq+value\n",
    "    if(value<thresh):\n",
    "        cnt=cnt+1\n",
    "        freq=freq+value\n",
    "    \n",
    "print(\"% of rare words in vocabulary:\",(cnt/tot_cnt)*100)\n",
    "print(\"Total Coverage of rare words:\",(freq/tot_freq)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare a tokenizer for reviews on training data\n",
    "x_tokenizer = Tokenizer(num_words=tot_cnt-cnt) \n",
    "x_tokenizer.fit_on_texts(list(x_tr))\n",
    "\n",
    "#convert text sequences into integer sequences\n",
    "x_tr_seq    =   x_tokenizer.texts_to_sequences(x_tr) \n",
    "x_val_seq   =   x_tokenizer.texts_to_sequences(x_val)\n",
    "\n",
    "#padding zero upto maximum length\n",
    "x_tr    =   pad_sequences(x_tr_seq,  maxlen=max_text_len, padding='post')\n",
    "x_val   =   pad_sequences(x_val_seq, maxlen=max_text_len, padding='post')\n",
    "\n",
    "#size of vocabulary ( +1 for padding token)\n",
    "x_voc   =  x_tokenizer.num_words + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "117009"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_voc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare a tokenizer for reviews on training data\n",
    "y_tokenizer = Tokenizer()   \n",
    "y_tokenizer.fit_on_texts(list(y_tr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "% of rare words in vocabulary: 74.6160369358334\n",
      "Total Coverage of rare words: 3.11067772226377\n"
     ]
    }
   ],
   "source": [
    "thresh=6\n",
    "\n",
    "cnt=0\n",
    "tot_cnt=0\n",
    "freq=0\n",
    "tot_freq=0\n",
    "\n",
    "for key,value in y_tokenizer.word_counts.items():\n",
    "    tot_cnt=tot_cnt+1\n",
    "    tot_freq=tot_freq+value\n",
    "    if(value<thresh):\n",
    "        cnt=cnt+1\n",
    "        freq=freq+value\n",
    "    \n",
    "print(\"% of rare words in vocabulary:\",(cnt/tot_cnt)*100)\n",
    "print(\"Total Coverage of rare words:\",(freq/tot_freq)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare a tokenizer for reviews on training data\n",
    "y_tokenizer = Tokenizer(num_words=tot_cnt-cnt) \n",
    "y_tokenizer.fit_on_texts(list(y_tr))\n",
    "\n",
    "#convert text sequences into integer sequences\n",
    "y_tr_seq    =   y_tokenizer.texts_to_sequences(y_tr) \n",
    "y_val_seq   =   y_tokenizer.texts_to_sequences(y_val) \n",
    "\n",
    "#padding zero upto maximum length\n",
    "y_tr    =   pad_sequences(y_tr_seq, maxlen=max_summary_len, padding='post')\n",
    "y_val   =   pad_sequences(y_val_seq, maxlen=max_summary_len, padding='post')\n",
    "\n",
    "#size of vocabulary\n",
    "y_voc  =   y_tokenizer.num_words +1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(384053, 384053)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_tokenizer.word_counts['sostok'],len(y_tr)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind=[]\n",
    "for i in range(len(y_tr)):\n",
    "    cnt=0\n",
    "    for j in y_tr[i]:\n",
    "        if j!=0:\n",
    "            cnt=cnt+1\n",
    "    if(cnt==2):\n",
    "        ind.append(i)\n",
    "\n",
    "y_tr=np.delete(y_tr,ind, axis=0)\n",
    "x_tr=np.delete(x_tr,ind, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind=[]\n",
    "for i in range(len(y_val)):\n",
    "    cnt=0\n",
    "    for j in y_val[i]:\n",
    "        if j!=0:\n",
    "            cnt=cnt+1\n",
    "    if(cnt==2):\n",
    "        ind.append(i)\n",
    "\n",
    "y_val=np.delete(y_val,ind, axis=0)\n",
    "x_val=np.delete(x_val,ind, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 200)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 200, 100)     11700900    input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     [(None, 200, 300), ( 481200      embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, 200, 300), ( 721200      lstm[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, None, 100)    2963500     input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   [(None, 200, 300), ( 721200      lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "lstm_3 (LSTM)                   [(None, None, 300),  481200      embedding_1[0][0]                \n",
      "                                                                 lstm_2[0][1]                     \n",
      "                                                                 lstm_2[0][2]                     \n",
      "__________________________________________________________________________________________________\n",
      "attention_layer (AttentionLayer [(None, None, 300),  180300      lstm_2[0][0]                     \n",
      "                                                                 lstm_3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "concat_layer (Concatenate)      (None, None, 600)    0           lstm_3[0][0]                     \n",
      "                                                                 attention_layer[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed (TimeDistribut (None, None, 29635)  17810635    concat_layer[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 35,060,135\n",
      "Trainable params: 35,060,135\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras import backend as K \n",
    "K.clear_session()\n",
    "\n",
    "latent_dim = 300\n",
    "embedding_dim=100\n",
    "\n",
    "# Encoder\n",
    "encoder_inputs = Input(shape=(max_text_len,))\n",
    "\n",
    "#embedding layer\n",
    "enc_emb =  Embedding(x_voc, embedding_dim,trainable=True)(encoder_inputs)\n",
    "\n",
    "#encoder lstm 1\n",
    "encoder_lstm1 = LSTM(latent_dim,return_sequences=True,return_state=True,dropout=0.4,recurrent_dropout=0.4)\n",
    "encoder_output1, state_h1, state_c1 = encoder_lstm1(enc_emb)\n",
    "\n",
    "#encoder lstm 2\n",
    "encoder_lstm2 = LSTM(latent_dim,return_sequences=True,return_state=True,dropout=0.4,recurrent_dropout=0.4)\n",
    "encoder_output2, state_h2, state_c2 = encoder_lstm2(encoder_output1)\n",
    "\n",
    "#encoder lstm 3\n",
    "encoder_lstm3=LSTM(latent_dim, return_state=True, return_sequences=True,dropout=0.4,recurrent_dropout=0.4)\n",
    "encoder_outputs, state_h, state_c= encoder_lstm3(encoder_output2)\n",
    "\n",
    "# Set up the decoder, using `encoder_states` as initial state.\n",
    "decoder_inputs = Input(shape=(None,))\n",
    "\n",
    "#embedding layer\n",
    "dec_emb_layer = Embedding(y_voc, embedding_dim,trainable=True)\n",
    "dec_emb = dec_emb_layer(decoder_inputs)\n",
    "\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True,dropout=0.4,recurrent_dropout=0.2)\n",
    "decoder_outputs,decoder_fwd_state, decoder_back_state = decoder_lstm(dec_emb,initial_state=[state_h, state_c])\n",
    "\n",
    "# Attention layer\n",
    "attn_layer = AttentionLayer(name='attention_layer')\n",
    "attn_out, attn_states = attn_layer([encoder_outputs, decoder_outputs])\n",
    "\n",
    "# Concat attention input and decoder LSTM output\n",
    "decoder_concat_input = Concatenate(axis=-1, name='concat_layer')([decoder_outputs, attn_out])\n",
    "\n",
    "#dense layer\n",
    "decoder_dense =  TimeDistributed(Dense(y_voc, activation='softmax'))\n",
    "decoder_outputs = decoder_dense(decoder_concat_input)\n",
    "\n",
    "# Define the model \n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "model.summary() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1,patience=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 383002 samples, validate on 42536 samples\n",
      "383002/383002 [==============================] - 4170s 11ms/sample - loss: nan - val_loss: nan\n"
     ]
    }
   ],
   "source": [
    "history=model.fit([x_tr,y_tr[:,:-1]], y_tr.reshape(y_tr.shape[0],y_tr.shape[1], 1)[:,1:] ,epochs=1,callbacks=[es],batch_size=128, validation_data=([x_val,y_val[:,:-1]], y_val.reshape(y_val.shape[0],y_val.shape[1], 1)[:,1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_dev_time = time.time() - start_time\n",
    "#print(\"--- %s seconds ---\" % model_dev_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot\n",
    "pyplot.plot(history.history['loss'], label='train')\n",
    "pyplot.plot(history.history['val_loss'], label='test')\n",
    "pyplot.legend()\n",
    "pyplot.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    # Encode the input as state vectors.\n",
    "    e_out, e_h, e_c = encoder_model.predict(input_seq)\n",
    "    \n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1,1))\n",
    "    \n",
    "    # Populate the first word of target sequence with the start word.\n",
    "    target_seq[0, 0] = target_word_index['sostok']\n",
    "\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "      \n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + [e_out, e_h, e_c])\n",
    "\n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_token = reverse_target_word_index[sampled_token_index]\n",
    "        \n",
    "        if(sampled_token!='eostok'):\n",
    "            decoded_sentence += ' '+sampled_token\n",
    "\n",
    "        # Exit condition: either hit max length or find stop word.\n",
    "        if (sampled_token == 'eostok'  or len(decoded_sentence.split()) >= (max_summary_len-1)):\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1,1))\n",
    "        target_seq[0, 0] = sampled_token_index\n",
    "\n",
    "        # Update internal states\n",
    "        e_h, e_c = h, c\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq2summary(input_seq):\n",
    "    newString=''\n",
    "    for i in input_seq:\n",
    "        if((i!=0 and i!=target_word_index['sostok']) and i!=target_word_index['eostok']):\n",
    "            newString=newString+reverse_target_word_index[i]+' '\n",
    "    return newString\n",
    "\n",
    "def seq2text(input_seq):\n",
    "    newString=''\n",
    "    for i in input_seq:\n",
    "        if(i!=0):\n",
    "            newString=newString+reverse_source_word_index[i]+' '\n",
    "    return newString\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_start_time = time.time()\n",
    "\n",
    "reverse_target_word_index=y_tokenizer.index_word\n",
    "reverse_source_word_index=x_tokenizer.index_word\n",
    "target_word_index=y_tokenizer.word_index\n",
    "# Encode the input sequence to get the feature vector\n",
    "encoder_model = Model(inputs=encoder_inputs,outputs=[encoder_outputs, state_h, state_c])\n",
    "\n",
    "# Decoder setup\n",
    "# Below tensors will hold the states of the previous time step\n",
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "decoder_hidden_state_input = Input(shape=(max_text_len,latent_dim))\n",
    "\n",
    "# Get the embeddings of the decoder sequence\n",
    "dec_emb2= dec_emb_layer(decoder_inputs) \n",
    "# To predict the next word in the sequence, set the initial states to the states from the previous time step\n",
    "decoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state=[decoder_state_input_h, decoder_state_input_c])\n",
    "\n",
    "#attention inference\n",
    "attn_out_inf, attn_states_inf = attn_layer([decoder_hidden_state_input, decoder_outputs2])\n",
    "decoder_inf_concat = Concatenate(axis=-1, name='concat')([decoder_outputs2, attn_out_inf])\n",
    "\n",
    "# A dense softmax layer to generate prob dist. over the target vocabulary\n",
    "decoder_outputs2 = decoder_dense(decoder_inf_concat) \n",
    "\n",
    "# Final decoder model\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + [decoder_hidden_state_input,decoder_state_input_h, decoder_state_input_c],\n",
    "    [decoder_outputs2] + [state_h2, state_c2])\n",
    "d = []\n",
    "for i in range(0,1):\n",
    "#    print(\"Review:\",seq2text(x_tr[i]))\n",
    "#    print(\"Original summary:\",seq2summary(y_tr[i]))\n",
    "#    print(\"Generated summary:\",decode_sequence(x_tr[i].reshape(1,max_text_len)))\n",
    "#    print(\"\\n\")\n",
    "    d.append((seq2summary(y_tr[i]), decode_sequence(x_tr[i].reshape(1,max_text_len))))\n",
    "\n",
    "total_inference_time = time.time() - inference_start_time\n",
    "print(\"Total Inference Time : \\n\")\n",
    "print(\"--- %s seconds ---\" % total_inference_time)\n",
    "pd.DataFrame(d, columns=('O_Summary', 'G_Summary'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "for i in range(0,40):\n",
    "    original = d[i][0].split()\n",
    "    generated = d[i][1].split()\n",
    "    reference = [original]\n",
    "    candidate = generated\n",
    "    score = sentence_bleu(reference, candidate)\n",
    "    print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install rouge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rouge import Rouge\n",
    "rouge = Rouge()\n",
    "scores = rouge.get_scores(d[5][1], d[5][0])\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
