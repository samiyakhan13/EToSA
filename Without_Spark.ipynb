{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Without Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bs4 in /usr/local/lib/python3.6/site-packages (0.0.1)\n",
      "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.6/site-packages (from bs4) (4.8.0)\n",
      "Requirement already satisfied: soupsieve>=1.2 in /usr/local/lib/python3.6/site-packages (from beautifulsoup4->bs4) (1.9.3)\n",
      "\u001b[31mmenpo 0.8.1 has requirement matplotlib<2.0,>=1.4, but you'll have matplotlib 3.0.2 which is incompatible.\u001b[0m\n",
      "\u001b[31mmenpo 0.8.1 has requirement pillow<5.0,>=3.0, but you'll have pillow 5.4.0 which is incompatible.\u001b[0m\n",
      "\u001b[31mmenpo 0.8.1 has requirement scipy<1.0,>=0.16, but you'll have scipy 1.2.0 which is incompatible.\u001b[0m\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 19.2.3 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pip\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/30/db/9e38760b32e3e7f40cce46dd5fb107b8c73840df38f0046d8e6514e675a1/pip-19.2.3-py2.py3-none-any.whl (1.4MB)\n",
      "\u001b[K    100% |████████████████████████████████| 1.4MB 17.8MB/s \n",
      "\u001b[31mmenpo 0.8.1 has requirement matplotlib<2.0,>=1.4, but you'll have matplotlib 3.0.2 which is incompatible.\u001b[0m\n",
      "\u001b[31mmenpo 0.8.1 has requirement pillow<5.0,>=3.0, but you'll have pillow 5.4.0 which is incompatible.\u001b[0m\n",
      "\u001b[31mmenpo 0.8.1 has requirement scipy<1.0,>=0.16, but you'll have scipy 1.2.0 which is incompatible.\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pip\n",
      "  Found existing installation: pip 10.0.1\n",
      "    Uninstalling pip-10.0.1:\n",
      "      Successfully uninstalled pip-10.0.1\n",
      "Successfully installed pip-19.2.3\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/site-packages (1.15.4)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/site-packages (3.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/site-packages (from matplotlib) (2.7.5)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/site-packages (from matplotlib) (0.10.0)\n",
      "Requirement already satisfied: numpy>=1.10.0 in /usr/local/lib/python3.6/site-packages (from matplotlib) (1.15.4)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/site-packages (from matplotlib) (2.3.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/site-packages (from matplotlib) (1.0.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/site-packages (from python-dateutil>=2.1->matplotlib) (1.12.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/site-packages (from kiwisolver>=1.0.1->matplotlib) (40.6.3)\n",
      "Requirement already satisfied: pillow in /usr/local/lib/python3.6/site-packages (5.4.0)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.6/site-packages (1.2.0)\n",
      "Requirement already satisfied: numpy>=1.8.2 in /usr/local/lib/python3.6/site-packages (from scipy) (1.15.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pip\n",
    "!pip install numpy\n",
    "!pip install matplotlib\n",
    "!pip install pillow\n",
    "!pip install scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from keras.preprocessing.text import Tokenizer \n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from nltk.corpus import stopwords\n",
    "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Concatenate, TimeDistributed\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import warnings\n",
    "pd.set_option(\"display.max_colwidth\", 200)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import nltk \n",
    "nltk.download('stopwords')\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "from tensorflow.python.keras.layers import Layer\n",
    "from tensorflow.python.keras import backend as K\n",
    "\n",
    "\n",
    "class AttentionLayer(Layer):\n",
    "    \"\"\"\n",
    "    This class implements Bahdanau attention (https://arxiv.org/pdf/1409.0473.pdf).\n",
    "    There are three sets of weights introduced W_a, U_a, and V_a\n",
    "     \"\"\"\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super(AttentionLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert isinstance(input_shape, list)\n",
    "        # Create a trainable weight variable for this layer.\n",
    "\n",
    "        self.W_a = self.add_weight(name='W_a',\n",
    "                                   shape=tf.TensorShape((input_shape[0][2], input_shape[0][2])),\n",
    "                                   initializer='uniform',\n",
    "                                   trainable=True)\n",
    "        self.U_a = self.add_weight(name='U_a',\n",
    "                                   shape=tf.TensorShape((input_shape[1][2], input_shape[0][2])),\n",
    "                                   initializer='uniform',\n",
    "                                   trainable=True)\n",
    "        self.V_a = self.add_weight(name='V_a',\n",
    "                                   shape=tf.TensorShape((input_shape[0][2], 1)),\n",
    "                                   initializer='uniform',\n",
    "                                   trainable=True)\n",
    "\n",
    "        super(AttentionLayer, self).build(input_shape)  # Be sure to call this at the end\n",
    "\n",
    "    def call(self, inputs, verbose=False):\n",
    "        \"\"\"\n",
    "        inputs: [encoder_output_sequence, decoder_output_sequence]\n",
    "        \"\"\"\n",
    "        assert type(inputs) == list\n",
    "        encoder_out_seq, decoder_out_seq = inputs\n",
    "        if verbose:\n",
    "            print('encoder_out_seq>', encoder_out_seq.shape)\n",
    "            print('decoder_out_seq>', decoder_out_seq.shape)\n",
    "\n",
    "        def energy_step(inputs, states):\n",
    "            \"\"\" Step function for computing energy for a single decoder state \"\"\"\n",
    "\n",
    "            assert_msg = \"States must be a list. However states {} is of type {}\".format(states, type(states))\n",
    "            assert isinstance(states, list) or isinstance(states, tuple), assert_msg\n",
    "\n",
    "            \"\"\" Some parameters required for shaping tensors\"\"\"\n",
    "            en_seq_len, en_hidden = encoder_out_seq.shape[1], encoder_out_seq.shape[2]\n",
    "            de_hidden = inputs.shape[-1]\n",
    "\n",
    "            \"\"\" Computing S.Wa where S=[s0, s1, ..., si]\"\"\"\n",
    "            # <= batch_size*en_seq_len, latent_dim\n",
    "            reshaped_enc_outputs = K.reshape(encoder_out_seq, (-1, en_hidden))\n",
    "            # <= batch_size*en_seq_len, latent_dim\n",
    "            W_a_dot_s = K.reshape(K.dot(reshaped_enc_outputs, self.W_a), (-1, en_seq_len, en_hidden))\n",
    "            if verbose:\n",
    "                print('wa.s>',W_a_dot_s.shape)\n",
    "\n",
    "            \"\"\" Computing hj.Ua \"\"\"\n",
    "            U_a_dot_h = K.expand_dims(K.dot(inputs, self.U_a), 1)  # <= batch_size, 1, latent_dim\n",
    "            if verbose:\n",
    "                print('Ua.h>',U_a_dot_h.shape)\n",
    "\n",
    "            \"\"\" tanh(S.Wa + hj.Ua) \"\"\"\n",
    "            # <= batch_size*en_seq_len, latent_dim\n",
    "            reshaped_Ws_plus_Uh = K.tanh(K.reshape(W_a_dot_s + U_a_dot_h, (-1, en_hidden)))\n",
    "            if verbose:\n",
    "                print('Ws+Uh>', reshaped_Ws_plus_Uh.shape)\n",
    "\n",
    "            \"\"\" softmax(va.tanh(S.Wa + hj.Ua)) \"\"\"\n",
    "            # <= batch_size, en_seq_len\n",
    "            e_i = K.reshape(K.dot(reshaped_Ws_plus_Uh, self.V_a), (-1, en_seq_len))\n",
    "            # <= batch_size, en_seq_len\n",
    "            e_i = K.softmax(e_i)\n",
    "\n",
    "            if verbose:\n",
    "                print('ei>', e_i.shape)\n",
    "\n",
    "            return e_i, [e_i]\n",
    "\n",
    "        def context_step(inputs, states):\n",
    "            \"\"\" Step function for computing ci using ei \"\"\"\n",
    "            # <= batch_size, hidden_size\n",
    "            c_i = K.sum(encoder_out_seq * K.expand_dims(inputs, -1), axis=1)\n",
    "            if verbose:\n",
    "                print('ci>', c_i.shape)\n",
    "            return c_i, [c_i]\n",
    "\n",
    "        def create_inital_state(inputs, hidden_size):\n",
    "            # We are not using initial states, but need to pass something to K.rnn funciton\n",
    "            fake_state = K.zeros_like(inputs)  # <= (batch_size, enc_seq_len, latent_dim\n",
    "            fake_state = K.sum(fake_state, axis=[1, 2])  # <= (batch_size)\n",
    "            fake_state = K.expand_dims(fake_state)  # <= (batch_size, 1)\n",
    "            fake_state = K.tile(fake_state, [1, hidden_size])  # <= (batch_size, latent_dim\n",
    "            return fake_state\n",
    "\n",
    "        fake_state_c = create_inital_state(encoder_out_seq, encoder_out_seq.shape[-1])\n",
    "        fake_state_e = create_inital_state(encoder_out_seq, encoder_out_seq.shape[1])  # <= (batch_size, enc_seq_len, latent_dim\n",
    "\n",
    "        \"\"\" Computing energy outputs \"\"\"\n",
    "        # e_outputs => (batch_size, de_seq_len, en_seq_len)\n",
    "        last_out, e_outputs, _ = K.rnn(\n",
    "            energy_step, decoder_out_seq, [fake_state_e],\n",
    "        )\n",
    "\n",
    "        \"\"\" Computing context vectors \"\"\"\n",
    "        last_out, c_outputs, _ = K.rnn(\n",
    "            context_step, e_outputs, [fake_state_c],\n",
    "        )\n",
    "\n",
    "        return c_outputs, e_outputs\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        \"\"\" Outputs produced by the layer \"\"\"\n",
    "        return [\n",
    "            tf.TensorShape((input_shape[1][0], input_shape[1][1], input_shape[1][2])),\n",
    "            tf.TensorShape((input_shape[1][0], input_shape[1][1], input_shape[0][1]))\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found GPU at: /device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "device_name = tf.test.gpu_device_name()\n",
    "if device_name != '/device:GPU:0':\n",
    "  raise SystemError('GPU device not found')\n",
    "print('Found GPU at: {}'.format(device_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.json\n",
      "18.json\n",
      "7.json\n",
      "17.json\n",
      "9.json\n",
      "5.json\n",
      "8.json\n",
      "1.json\n",
      "2.json\n",
      "6.json\n",
      "19.json\n",
      "12.json\n",
      "14.json\n",
      "16.json\n",
      "25.json\n",
      "23.json\n",
      "28.json\n",
      "24.json\n",
      "31.json\n",
      "27.json\n",
      "21.json\n",
      "29.json\n",
      "34.json\n",
      "35.json\n",
      "33.json\n",
      "36.json\n",
      "40.json\n",
      "39.json\n",
      "50.json\n",
      "48.json\n",
      "46.json\n",
      "49.json\n",
      "42.json\n",
      "57.json\n",
      "69.json\n",
      "60.json\n",
      "56.json\n",
      "70.json\n",
      "55.json\n",
      "71.json\n",
      "52.json\n",
      "68.json\n",
      "59.json\n",
      "53.json\n",
      "65.json\n",
      "51.json\n",
      "58.json\n",
      "Data Ingestion Time: \n",
      "\n",
      "--- 32698.9160656929 seconds ---\n",
      "Number of Ingested Records: \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "summary    1036641\n",
       "text        555785\n",
       "dtype: int64"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import os.path\n",
    "from glob import glob\n",
    "\n",
    "ingestion_start_time = time.time()\n",
    "\n",
    "cols = ['summary', 'text']\n",
    "data = pd.DataFrame(columns=cols)\n",
    "\n",
    "for file_name in os.listdir(\"/floyd/input/sdata250gb/\"):\n",
    "    if file_name != '._1.json':\n",
    "        with open(\"/floyd/input/sdata250gb/\"+file_name) as f:\n",
    "            print(file_name)\n",
    "            for line in f:\n",
    "                data1=json.loads(line)\n",
    "                lst_dict=({'summary':data1['title'], 'text':data1['abstract']})\n",
    "                data = data.append(lst_dict, ignore_index=True)\n",
    "for file_name in os.listdir(\"/floyd/input/sdata250gb1/\"):\n",
    "    with open(\"/floyd/input/sdata250gb1/\"+file_name) as f:\n",
    "        print(file_name)\n",
    "        for line in f:\n",
    "            data1=json.loads(line)\n",
    "            lst_dict=({'summary':data1['title'], 'text':data1['abstract']})\n",
    "            data = data.append(lst_dict, ignore_index=True)\n",
    "for file_name in os.listdir(\"/floyd/input/sdata250gb3/\"):\n",
    "    with open(\"/floyd/input/sdata250gb3/\"+file_name) as f:\n",
    "        print(file_name)\n",
    "        for line in f:\n",
    "            data1=json.loads(line)\n",
    "            lst_dict=({'summary':data1['title'], 'text':data1['abstract']})\n",
    "            data = data.append(lst_dict, ignore_index=True)\n",
    "for file_name in os.listdir(\"/floyd/input/sdata250gb4/\"):\n",
    "    with open(\"/floyd/input/sdata250gb4/\"+file_name) as f:\n",
    "        print(file_name)\n",
    "        for line in f:\n",
    "            data1=json.loads(line)\n",
    "            lst_dict=({'summary':data1['title'], 'text':data1['abstract']})\n",
    "            data = data.append(lst_dict, ignore_index=True)\n",
    "for file_name in os.listdir(\"/floyd/input/sdata250gb5/\"):\n",
    "    if file_name != '._65.json':\n",
    "        with open(\"/floyd/input/sdata250gb5/\"+file_name) as f:\n",
    "            print(file_name)\n",
    "            for line in f:\n",
    "                data1=json.loads(line)\n",
    "                lst_dict=({'summary':data1['title'], 'text':data1['abstract']})\n",
    "                data = data.append(lst_dict, ignore_index=True)\n",
    "\n",
    "                \n",
    "ingestion_time = time.time() - ingestion_start_time\n",
    "print(\"Data Ingestion Time: \\n\")\n",
    "print(\"--- %s seconds ---\" % ingestion_time)\n",
    "print(\"Number of Ingested Records: \\n\")\n",
    "data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "contraction_mapping = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\",\n",
    "                           \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\",\n",
    "                           \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",\n",
    "                           \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\",\n",
    "                           \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\",\n",
    "                           \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\",\n",
    "                           \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\",\n",
    "                           \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\",\n",
    "                           \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\",\n",
    "                           \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\",\n",
    "                           \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\",\n",
    "                           \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\",\n",
    "                           \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\",\n",
    "                           \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\",\n",
    "                           \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\",\n",
    "                           \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",\n",
    "                           \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\",\n",
    "                           \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\",\n",
    "                           \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\",\n",
    "                           \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\",\n",
    "                           \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\n",
    "                           \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\",\n",
    "                           \"you're\": \"you are\", \"you've\": \"you have\"}\n",
    "stop_words = set(stopwords.words('english')) \n",
    "\n",
    "def text_cleaner(text,num):\n",
    "    newString = text.lower()\n",
    "    newString = BeautifulSoup(newString, \"html\").text\n",
    "    newString = re.sub(r'\\([^)]*\\)', '', newString)\n",
    "    newString = re.sub('\"','', newString)\n",
    "    newString = ' '.join([contraction_mapping[t] if t in contraction_mapping else t for t in newString.split(\" \")])    \n",
    "    newString = re.sub(r\"'s\\b\",\"\",newString)\n",
    "    newString = re.sub(\"[^a-zA-Z]\", \" \", newString) \n",
    "    newString = re.sub('[m]{2,}', 'mm', newString)\n",
    "    if(num==0):\n",
    "        tokens = [w for w in newString.split() if not w in stop_words]\n",
    "    else:\n",
    "        tokens=newString.split()\n",
    "    long_words=[]\n",
    "    for i in tokens:\n",
    "        if len(i)>1:                                                 #removing short word\n",
    "            long_words.append(i)   \n",
    "    return (\" \".join(long_words)).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-Cleaning Time : \n",
      "\n",
      "--- 1.066657543182373 seconds ---\n",
      "Cleaning Time : \n",
      "\n",
      "--- 862.4533414840698 seconds ---\n",
      "Post-Cleaning Time : \n",
      "\n",
      "--- 0.8872110843658447 seconds ---\n",
      "Total Preprocessing Time: \n",
      "\n",
      "--- 864.4089729785919 seconds ---\n"
     ]
    }
   ],
   "source": [
    "preprocess_start_time = time.time()\n",
    "#data.drop_duplicates(subset=['title'],inplace=True)#dropping duplicates\n",
    "data.drop_duplicates(subset=['text'],inplace=True)#dropping duplicates\n",
    "data.dropna(axis=0,inplace=True)#dropping na\n",
    "preclean_time = time.time() - preprocess_start_time\n",
    "print(\"Pre-Cleaning Time : \\n\")\n",
    "print(\"--- %s seconds ---\" % preclean_time)\n",
    "clean_start_time = time.time()\n",
    "#call the function\n",
    "cleaned_text = []\n",
    "for t in data['text']:\n",
    "    cleaned_text.append(text_cleaner(t,0)) \n",
    "#call the function\n",
    "cleaned_summary = []\n",
    "for t in data['summary']:\n",
    "    cleaned_summary.append(text_cleaner(t,1))\n",
    "clean_time = time.time() - clean_start_time\n",
    "print(\"Cleaning Time : \\n\")\n",
    "print(\"--- %s seconds ---\" % clean_time)\n",
    "postclean_start_time = time.time()\n",
    "data['cleaned_text']=cleaned_text\n",
    "data['cleaned_summary']=cleaned_summary\n",
    "data.replace('', np.nan, inplace=True)\n",
    "data.dropna(axis=0,inplace=True)\n",
    "postclean_time = time.time() - postclean_start_time\n",
    "print(\"Post-Cleaning Time : \\n\")\n",
    "print(\"--- %s seconds ---\" % postclean_time)\n",
    "total_preprocess_time = time.time() - preprocess_start_time\n",
    "print(\"Total Preprocessing Time: \\n\")\n",
    "print(\"--- %s seconds ---\" % total_preprocess_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "summary            480713\n",
       "text               480713\n",
       "cleaned_text       480713\n",
       "cleaned_summary    480713\n",
       "dtype: int64"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "datay = data.sort_values(by='summary', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "datay.to_csv('withoutspark_5.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "text_word_count = []\n",
    "summary_word_count = []\n",
    "\n",
    "# populate the lists with sentence lengths\n",
    "for i in data['cleaned_text']:\n",
    "      text_word_count.append(len(i.split()))\n",
    "\n",
    "for i in data['cleaned_summary']:\n",
    "      summary_word_count.append(len(i.split()))\n",
    "\n",
    "length_df = pd.DataFrame({'text':text_word_count, 'summary':summary_word_count})\n",
    "\n",
    "length_df.hist(bins = 30)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9469163219064582\n",
      "0.9306158337936399\n"
     ]
    }
   ],
   "source": [
    "cnt=0\n",
    "for i in data['cleaned_summary']:\n",
    "    if(len(i.split())<=20):\n",
    "        cnt=cnt+1\n",
    "print(cnt/len(data['cleaned_summary']))\n",
    "cnt=0\n",
    "for i in data['cleaned_text']:\n",
    "    if(len(i.split())<=200):\n",
    "        cnt=cnt+1\n",
    "print(cnt/len(data['cleaned_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_text_len=200\n",
    "max_summary_len=20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_text =np.array(data['cleaned_text'])\n",
    "cleaned_summary=np.array(data['cleaned_summary'])\n",
    "\n",
    "short_text=[]\n",
    "short_summary=[]\n",
    "\n",
    "for i in range(len(cleaned_text)):\n",
    "    if(len(cleaned_summary[i].split())<=max_summary_len and len(cleaned_text[i].split())<=max_text_len):\n",
    "        short_text.append(cleaned_text[i])\n",
    "        short_summary.append(cleaned_summary[i])\n",
    "        \n",
    "df=pd.DataFrame({'text':short_text,'summary':short_summary})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['summary'] = df['summary'].apply(lambda x : 'sostok '+ x + ' eostok')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_tr,x_val,y_tr,y_val=train_test_split(np.array(df['text']),np.array(df['summary']),test_size=0.1,random_state=0,shuffle=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer \n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "#prepare a tokenizer for reviews on training data\n",
    "x_tokenizer = Tokenizer() \n",
    "x_tokenizer.fit_on_texts(list(x_tr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "% of rare words in vocabulary: 61.98014386959064\n",
      "Total Coverage of rare words: 1.7346652959887139\n"
     ]
    }
   ],
   "source": [
    "thresh=4\n",
    "\n",
    "cnt=0\n",
    "tot_cnt=0\n",
    "freq=0\n",
    "tot_freq=0\n",
    "\n",
    "for key,value in x_tokenizer.word_counts.items():\n",
    "    tot_cnt=tot_cnt+1\n",
    "    tot_freq=tot_freq+value\n",
    "    if(value<thresh):\n",
    "        cnt=cnt+1\n",
    "        freq=freq+value\n",
    "    \n",
    "print(\"% of rare words in vocabulary:\",(cnt/tot_cnt)*100)\n",
    "print(\"Total Coverage of rare words:\",(freq/tot_freq)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare a tokenizer for reviews on training data\n",
    "x_tokenizer = Tokenizer(num_words=tot_cnt-cnt) \n",
    "x_tokenizer.fit_on_texts(list(x_tr))\n",
    "\n",
    "#convert text sequences into integer sequences\n",
    "x_tr_seq    =   x_tokenizer.texts_to_sequences(x_tr) \n",
    "x_val_seq   =   x_tokenizer.texts_to_sequences(x_val)\n",
    "\n",
    "#padding zero upto maximum length\n",
    "x_tr    =   pad_sequences(x_tr_seq,  maxlen=max_text_len, padding='post')\n",
    "x_val   =   pad_sequences(x_val_seq, maxlen=max_text_len, padding='post')\n",
    "\n",
    "#size of vocabulary ( +1 for padding token)\n",
    "x_voc   =  x_tokenizer.num_words + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50053"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_voc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare a tokenizer for reviews on training data\n",
    "y_tokenizer = Tokenizer()   \n",
    "y_tokenizer.fit_on_texts(list(y_tr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "% of rare words in vocabulary: 76.16699260266984\n",
      "Total Coverage of rare words: 6.659784456097744\n"
     ]
    }
   ],
   "source": [
    "thresh=6\n",
    "\n",
    "cnt=0\n",
    "tot_cnt=0\n",
    "freq=0\n",
    "tot_freq=0\n",
    "\n",
    "for key,value in y_tokenizer.word_counts.items():\n",
    "    tot_cnt=tot_cnt+1\n",
    "    tot_freq=tot_freq+value\n",
    "    if(value<thresh):\n",
    "        cnt=cnt+1\n",
    "        freq=freq+value\n",
    "    \n",
    "print(\"% of rare words in vocabulary:\",(cnt/tot_cnt)*100)\n",
    "print(\"Total Coverage of rare words:\",(freq/tot_freq)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare a tokenizer for reviews on training data\n",
    "y_tokenizer = Tokenizer(num_words=tot_cnt-cnt) \n",
    "y_tokenizer.fit_on_texts(list(y_tr))\n",
    "\n",
    "#convert text sequences into integer sequences\n",
    "y_tr_seq    =   y_tokenizer.texts_to_sequences(y_tr) \n",
    "y_val_seq   =   y_tokenizer.texts_to_sequences(y_val) \n",
    "\n",
    "#padding zero upto maximum length\n",
    "y_tr    =   pad_sequences(y_tr_seq, maxlen=max_summary_len, padding='post')\n",
    "y_val   =   pad_sequences(y_val_seq, maxlen=max_summary_len, padding='post')\n",
    "\n",
    "#size of vocabulary\n",
    "y_voc  =   y_tokenizer.num_words +1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(70726, 70726)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_tokenizer.word_counts['sostok'],len(y_tr)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind=[]\n",
    "for i in range(len(y_tr)):\n",
    "    cnt=0\n",
    "    for j in y_tr[i]:\n",
    "        if j!=0:\n",
    "            cnt=cnt+1\n",
    "    if(cnt==2):\n",
    "        ind.append(i)\n",
    "\n",
    "y_tr=np.delete(y_tr,ind, axis=0)\n",
    "x_tr=np.delete(x_tr,ind, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind=[]\n",
    "for i in range(len(y_val)):\n",
    "    cnt=0\n",
    "    for j in y_val[i]:\n",
    "        if j!=0:\n",
    "            cnt=cnt+1\n",
    "    if(cnt==2):\n",
    "        ind.append(i)\n",
    "\n",
    "y_val=np.delete(y_val,ind, axis=0)\n",
    "x_val=np.delete(x_val,ind, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/site-packages/tensorflow/python/keras/backend.py:4010: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 200)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 200, 100)     5005300     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     [(None, 200, 300), ( 481200      embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, 200, 300), ( 721200      lstm[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, None, 100)    1121300     input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   [(None, 200, 300), ( 721200      lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "lstm_3 (LSTM)                   [(None, None, 300),  481200      embedding_1[0][0]                \n",
      "                                                                 lstm_2[0][1]                     \n",
      "                                                                 lstm_2[0][2]                     \n",
      "__________________________________________________________________________________________________\n",
      "attention_layer (AttentionLayer [(None, None, 300),  180300      lstm_2[0][0]                     \n",
      "                                                                 lstm_3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "concat_layer (Concatenate)      (None, None, 600)    0           lstm_3[0][0]                     \n",
      "                                                                 attention_layer[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed (TimeDistribut (None, None, 11213)  6739013     concat_layer[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 15,450,713\n",
      "Trainable params: 15,450,713\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras import backend as K \n",
    "K.clear_session()\n",
    "\n",
    "latent_dim = 300\n",
    "embedding_dim=100\n",
    "\n",
    "# Encoder\n",
    "encoder_inputs = Input(shape=(max_text_len,))\n",
    "\n",
    "#embedding layer\n",
    "enc_emb =  Embedding(x_voc, embedding_dim,trainable=True)(encoder_inputs)\n",
    "\n",
    "#encoder lstm 1\n",
    "encoder_lstm1 = LSTM(latent_dim,return_sequences=True,return_state=True,dropout=0.4,recurrent_dropout=0.4)\n",
    "encoder_output1, state_h1, state_c1 = encoder_lstm1(enc_emb)\n",
    "\n",
    "#encoder lstm 2\n",
    "encoder_lstm2 = LSTM(latent_dim,return_sequences=True,return_state=True,dropout=0.4,recurrent_dropout=0.4)\n",
    "encoder_output2, state_h2, state_c2 = encoder_lstm2(encoder_output1)\n",
    "\n",
    "#encoder lstm 3\n",
    "encoder_lstm3=LSTM(latent_dim, return_state=True, return_sequences=True,dropout=0.4,recurrent_dropout=0.4)\n",
    "encoder_outputs, state_h, state_c= encoder_lstm3(encoder_output2)\n",
    "\n",
    "# Set up the decoder, using `encoder_states` as initial state.\n",
    "decoder_inputs = Input(shape=(None,))\n",
    "\n",
    "#embedding layer\n",
    "dec_emb_layer = Embedding(y_voc, embedding_dim,trainable=True)\n",
    "dec_emb = dec_emb_layer(decoder_inputs)\n",
    "\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True,dropout=0.4,recurrent_dropout=0.2)\n",
    "decoder_outputs,decoder_fwd_state, decoder_back_state = decoder_lstm(dec_emb,initial_state=[state_h, state_c])\n",
    "\n",
    "# Attention layer\n",
    "attn_layer = AttentionLayer(name='attention_layer')\n",
    "attn_out, attn_states = attn_layer([encoder_outputs, decoder_outputs])\n",
    "\n",
    "# Concat attention input and decoder LSTM output\n",
    "decoder_concat_input = Concatenate(axis=-1, name='concat_layer')([decoder_outputs, attn_out])\n",
    "\n",
    "#dense layer\n",
    "decoder_dense =  TimeDistributed(Dense(y_voc, activation='softmax'))\n",
    "decoder_outputs = decoder_dense(decoder_concat_input)\n",
    "\n",
    "# Define the model \n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "model.summary() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1,patience=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 70513 samples, validate on 7830 samples\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "70513/70513 [==============================] - 1072s 15ms/sample - loss: 3.9179 - val_loss: 3.6273\n"
     ]
    }
   ],
   "source": [
    "history=model.fit([x_tr,y_tr[:,:-1]], y_tr.reshape(y_tr.shape[0],y_tr.shape[1], 1)[:,1:] ,epochs=1,callbacks=[es],batch_size=128, validation_data=([x_val,y_val[:,:-1]], y_val.reshape(y_val.shape[0],y_val.shape[1], 1)[:,1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_dev_time = time.time() - start_time\n",
    "#print(\"--- %s seconds ---\" % model_dev_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot\n",
    "pyplot.plot(history.history['loss'], label='train')\n",
    "pyplot.plot(history.history['val_loss'], label='test')\n",
    "pyplot.legend()\n",
    "pyplot.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    # Encode the input as state vectors.\n",
    "    e_out, e_h, e_c = encoder_model.predict(input_seq)\n",
    "    \n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1,1))\n",
    "    \n",
    "    # Populate the first word of target sequence with the start word.\n",
    "    target_seq[0, 0] = target_word_index['sostok']\n",
    "\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "      \n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + [e_out, e_h, e_c])\n",
    "\n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_token = reverse_target_word_index[sampled_token_index]\n",
    "        \n",
    "        if(sampled_token!='eostok'):\n",
    "            decoded_sentence += ' '+sampled_token\n",
    "\n",
    "        # Exit condition: either hit max length or find stop word.\n",
    "        if (sampled_token == 'eostok'  or len(decoded_sentence.split()) >= (max_summary_len-1)):\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1,1))\n",
    "        target_seq[0, 0] = sampled_token_index\n",
    "\n",
    "        # Update internal states\n",
    "        e_h, e_c = h, c\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq2summary(input_seq):\n",
    "    newString=''\n",
    "    for i in input_seq:\n",
    "        if((i!=0 and i!=target_word_index['sostok']) and i!=target_word_index['eostok']):\n",
    "            newString=newString+reverse_target_word_index[i]+' '\n",
    "    return newString\n",
    "\n",
    "def seq2text(input_seq):\n",
    "    newString=''\n",
    "    for i in input_seq:\n",
    "        if(i!=0):\n",
    "            newString=newString+reverse_source_word_index[i]+' '\n",
    "    return newString\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Inference Time : \n",
      "\n",
      "--- 2.372572898864746 seconds ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>O_Summary</th>\n",
       "      <th>G_Summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>modelling and control of hybrid stepping motor</td>\n",
       "      <td>the effect of the effect of the effect of the effect of the effect of the effect of the</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         O_Summary  \\\n",
       "0  modelling and control of hybrid stepping motor    \n",
       "\n",
       "                                                                                  G_Summary  \n",
       "0   the effect of the effect of the effect of the effect of the effect of the effect of the  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inference_start_time = time.time()\n",
    "\n",
    "reverse_target_word_index=y_tokenizer.index_word\n",
    "reverse_source_word_index=x_tokenizer.index_word\n",
    "target_word_index=y_tokenizer.word_index\n",
    "# Encode the input sequence to get the feature vector\n",
    "encoder_model = Model(inputs=encoder_inputs,outputs=[encoder_outputs, state_h, state_c])\n",
    "\n",
    "# Decoder setup\n",
    "# Below tensors will hold the states of the previous time step\n",
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "decoder_hidden_state_input = Input(shape=(max_text_len,latent_dim))\n",
    "\n",
    "# Get the embeddings of the decoder sequence\n",
    "dec_emb2= dec_emb_layer(decoder_inputs) \n",
    "# To predict the next word in the sequence, set the initial states to the states from the previous time step\n",
    "decoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state=[decoder_state_input_h, decoder_state_input_c])\n",
    "\n",
    "#attention inference\n",
    "attn_out_inf, attn_states_inf = attn_layer([decoder_hidden_state_input, decoder_outputs2])\n",
    "decoder_inf_concat = Concatenate(axis=-1, name='concat')([decoder_outputs2, attn_out_inf])\n",
    "\n",
    "# A dense softmax layer to generate prob dist. over the target vocabulary\n",
    "decoder_outputs2 = decoder_dense(decoder_inf_concat) \n",
    "\n",
    "# Final decoder model\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + [decoder_hidden_state_input,decoder_state_input_h, decoder_state_input_c],\n",
    "    [decoder_outputs2] + [state_h2, state_c2])\n",
    "d = []\n",
    "for i in range(0,1):\n",
    "#    print(\"Review:\",seq2text(x_tr[i]))\n",
    "#    print(\"Original summary:\",seq2summary(y_tr[i]))\n",
    "#    print(\"Generated summary:\",decode_sequence(x_tr[i].reshape(1,max_text_len)))\n",
    "#    print(\"\\n\")\n",
    "    d.append((seq2summary(y_tr[i]), decode_sequence(x_tr[i].reshape(1,max_text_len))))\n",
    "\n",
    "total_inference_time = time.time() - inference_start_time\n",
    "print(\"Total Inference Time : \\n\")\n",
    "print(\"--- %s seconds ---\" % total_inference_time)\n",
    "pd.DataFrame(d, columns=('O_Summary', 'G_Summary'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "for i in range(0,40):\n",
    "    original = d[i][0].split()\n",
    "    generated = d[i][1].split()\n",
    "    reference = [original]\n",
    "    candidate = generated\n",
    "    score = sentence_bleu(reference, candidate)\n",
    "    print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install rouge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rouge import Rouge\n",
    "rouge = Rouge()\n",
    "scores = rouge.get_scores(d[5][1], d[5][0])\n",
    "scores"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
